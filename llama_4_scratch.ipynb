{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce245952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM FROM SCRATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4524bec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a Byte Pair Encoding (BPE) Tokenizer from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa67fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare training data\n",
    "# Note: here we are NOT training a neural network.\n",
    "# We are just creating a list of tokens from text.\n",
    "#\n",
    "# What we need: a text corpus.\n",
    "# The tokenizer will learn \"merge rules\" based on how often pairs of characters appear.\n",
    "#\n",
    "# Example:\n",
    "#   \"i\"   -> token 1\n",
    "#   \"s\"   -> token 2\n",
    "#   \"is\"  -> token 3\n",
    "#\n",
    "# How it works:\n",
    "# 1. Start with text split into individual characters.\n",
    "#    (every character is always in the vocabulary)\n",
    "# 2. Count how often character pairs occur.\n",
    "# 3. Merge the most frequent pairs into new tokens (subwords).\n",
    "# 4. Over time, the tokenizer builds a vocabulary that mixes\n",
    "#    single characters + useful subword tokens.\n",
    "#\n",
    "# Important:\n",
    "# - Characters always remain as fallback tokens.\n",
    "# - Subwords get priority when tokenizing, making it more efficient.\n",
    "#\n",
    "# Concretely:\n",
    "# - We scan the text and count how many times each pair of characters appears.\n",
    "# - For example, if the pair \"is\" appears very often, we create a new token for it.\n",
    "# - This reduces computation: instead of processing \"i\" and \"s\" separately,\n",
    "#   we can treat \"is\" as a single token.\n",
    "# - Note: \"i\" and \"s\" still remain in the vocabulary as individual tokens,\n",
    "#   but whenever the pair \"is\" exists, it takes priority over the single characters.\n",
    "#\n",
    "# Iterative merges:\n",
    "# - The process continues on top of previously created tokens.\n",
    "# - For example, if \"is\" was already merged into a token, and we notice \"his\"\n",
    "#   appears frequently, we merge \"h\" + \"is\" → \"his\".\n",
    "# - Next, if \"this\" is common, we merge \"t\" + \"his\" → \"this\".\n",
    "# - This way, the vocabulary gradually grows from characters → subwords → whole words,\n",
    "#   depending on frequency in the training text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17effa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our corpus of data\n",
    "corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "984a2fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: \n",
      "This is the first document.\n",
      "This document is the second document.\n",
      "And this is the third one.\n",
      "Is this the first document?\n"
     ]
    }
   ],
   "source": [
    "print(\"Corpus: \")\n",
    "for doc in corpus:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dca3915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Initialize vocabulary with unique characters\n",
    "#\n",
    "# The first version of our vocabulary is simply all the unique characters\n",
    "# that appear in the training corpus.\n",
    "# Each character will be treated as an initial token.\n",
    "#\n",
    "# In addition, we add a special end-of-word marker (</w>).\n",
    "# This marker helps the tokenizer know where words end, so that\n",
    "# frequent whole words or subwords can be merged properly later.\n",
    "#\n",
    "# Example:\n",
    "#   \"this\"  →  [\"t\", \"h\", \"i\", \"s</w>\"]\n",
    "#   \"is\"    →  [\"i\", \"s</w>\"]\n",
    "#\n",
    "unique_chars = set()\n",
    "for doc in corpus:\n",
    "    for char in doc:\n",
    "        unique_chars.add(char)\n",
    "\n",
    "# Convert to a sorted list so the vocabulary is consistent and reproducible\n",
    "vocab = list(unique_chars)\n",
    "vocab.sort()\n",
    "\n",
    "# Add the special end-of-word token\n",
    "end_of_word = \"</w>\"\n",
    "vocab.append(end_of_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed80eff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Vocabulary:\n",
      "[' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>']\n",
      "Vocabulary Size: 20\n"
     ]
    }
   ],
   "source": [
    "print(\"Initial Vocabulary:\")\n",
    "print(vocab)\n",
    "print(f\"Vocabulary Size: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a386ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pre-tokenized Word Frequencies:\n",
      "{('T', 'h', 'i', 's', '</w>'): 2, ('i', 's', '</w>'): 3, ('t', 'h', 'e', '</w>'): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('t', 'h', 'i', 's', '</w>'): 2, ('t', 'h', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's', '</w>'): 1, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Pre-tokenize the corpus\n",
    "#\n",
    "# Goal:\n",
    "# - Split text into words (by spaces, for simplicity).\n",
    "# - Break each word into its characters.\n",
    "# - Add the special end-of-word token (</w>) at the end of every word.\n",
    "#\n",
    "# Why?\n",
    "# - This gives us the initial representation of words as sequences of characters.\n",
    "# - Example: \"This\" → (\"T\", \"h\", \"i\", \"s\", \"</w>\")\n",
    "#\n",
    "# Implementation details:\n",
    "# - We store each word as a tuple of characters (immutable).\n",
    "#   Tuples can be used as dictionary keys, unlike lists.\n",
    "# - We count how many times each word (as a sequence of characters) appears\n",
    "#   in the whole corpus.\n",
    "#\n",
    "# Note:\n",
    "# - Adding the </w> token ensures that subwords are learned within word\n",
    "#   boundaries. For example:\n",
    "#   \"document\" → (\"d\", \"o\", \"c\", \"u\", \"m\", \"e\", \"n\", \"t\", \"</w>\")\n",
    "#   This way, if \"doc\" becomes a frequent subword, it is clear that it\n",
    "#   belongs inside the word \"document\" and not across words.\n",
    "#\n",
    "word_splits = {}\n",
    "for doc in corpus:\n",
    "    words = doc.split(' ')\n",
    "    for word in words:\n",
    "        if word:\n",
    "            # Represent word as characters + </w>\n",
    "            char_list = list(word) + [end_of_word]\n",
    "            word_tuple = tuple(char_list)\n",
    "            \n",
    "            # Count frequency of this word form\n",
    "            if word_tuple not in word_splits:\n",
    "                word_splits[word_tuple] = 0\n",
    "            word_splits[word_tuple] += 1\n",
    "\n",
    "print(\"\\nPre-tokenized Word Frequencies:\")\n",
    "print(word_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ac0604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method keys of dict object at 0x000001D482CA0940>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec214d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
