{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce245952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM FROM SCRATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4524bec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a Byte Pair Encoding (BPE) Tokenizer from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa67fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare training data\n",
    "# Note: here we are NOT training a neural network.\n",
    "# We are just creating a list of tokens from text.\n",
    "#\n",
    "# What we need: a text corpus.\n",
    "# The tokenizer will learn \"merge rules\" based on how often pairs of characters appear.\n",
    "#\n",
    "# Example:\n",
    "#   \"i\"   -> token 1\n",
    "#   \"s\"   -> token 2\n",
    "#   \"is\"  -> token 3\n",
    "#\n",
    "# How it works:\n",
    "# 1. Start with text split into individual characters.\n",
    "#    (every character is always in the vocabulary)\n",
    "# 2. Count how often character pairs occur.\n",
    "# 3. Merge the most frequent pairs into new tokens (subwords).\n",
    "# 4. Over time, the tokenizer builds a vocabulary that mixes\n",
    "#    single characters + useful subword tokens.\n",
    "#\n",
    "# Important:\n",
    "# - Characters always remain as fallback tokens.\n",
    "# - Subwords get priority when tokenizing, making it more efficient.\n",
    "#\n",
    "# Concretely:\n",
    "# - We scan the text and count how many times each pair of characters appears.\n",
    "# - For example, if the pair \"is\" appears very often, we create a new token for it.\n",
    "# - This reduces computation: instead of processing \"i\" and \"s\" separately,\n",
    "#   we can treat \"is\" as a single token.\n",
    "# - Note: \"i\" and \"s\" still remain in the vocabulary as individual tokens,\n",
    "#   but whenever the pair \"is\" exists, it takes priority over the single characters.\n",
    "#\n",
    "# Iterative merges:\n",
    "# - The process continues on top of previously created tokens.\n",
    "# - For example, if \"is\" was already merged into a token, and we notice \"his\"\n",
    "#   appears frequently, we merge \"h\" + \"is\" → \"his\".\n",
    "# - Next, if \"this\" is common, we merge \"t\" + \"his\" → \"this\".\n",
    "# - This way, the vocabulary gradually grows from characters → subwords → whole words,\n",
    "#   depending on frequency in the training text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17effa38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
