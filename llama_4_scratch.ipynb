{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce245952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM FROM SCRATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4524bec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a Byte Pair Encoding (BPE) Tokenizer from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfa67fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare training data\n",
    "# Note: here we are NOT training a neural network.\n",
    "# We are just creating a list of tokens from text.\n",
    "#\n",
    "# What we need: a text corpus.\n",
    "# The tokenizer will learn \"merge rules\" based on how often pairs of characters appear.\n",
    "#\n",
    "# Example:\n",
    "#   \"i\"   -> token 1\n",
    "#   \"s\"   -> token 2\n",
    "#   \"is\"  -> token 3\n",
    "#\n",
    "# How it works:\n",
    "# 1. Start with text split into individual characters.\n",
    "#    (every character is always in the vocabulary)\n",
    "# 2. Count how often character pairs occur.\n",
    "# 3. Merge the most frequent pairs into new tokens (subwords).\n",
    "# 4. Over time, the tokenizer builds a vocabulary that mixes\n",
    "#    single characters + useful subword tokens.\n",
    "#\n",
    "# Important:\n",
    "# - Characters always remain as fallback tokens.\n",
    "# - Subwords get priority when tokenizing, making it more efficient.\n",
    "#\n",
    "# Concretely:\n",
    "# - We scan the text and count how many times each pair of characters appears.\n",
    "# - For example, if the pair \"is\" appears very often, we create a new token for it.\n",
    "# - This reduces computation: instead of processing \"i\" and \"s\" separately,\n",
    "#   we can treat \"is\" as a single token.\n",
    "# - Note: \"i\" and \"s\" still remain in the vocabulary as individual tokens,\n",
    "#   but whenever the pair \"is\" exists, it takes priority over the single characters.\n",
    "#\n",
    "# Iterative merges:\n",
    "# - The process continues on top of previously created tokens.\n",
    "# - For example, if \"is\" was already merged into a token, and we notice \"his\"\n",
    "#   appears frequently, we merge \"h\" + \"is\" → \"his\".\n",
    "# - Next, if \"this\" is common, we merge \"t\" + \"his\" → \"this\".\n",
    "# - This way, the vocabulary gradually grows from characters → subwords → whole words,\n",
    "#   depending on frequency in the training text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17effa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our corpus of data\n",
    "corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "984a2fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: \n",
      "This is the first document.\n",
      "This document is the second document.\n",
      "And this is the third one.\n",
      "Is this the first document?\n"
     ]
    }
   ],
   "source": [
    "print(\"Corpus: \")\n",
    "for doc in corpus:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dca3915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Initialize vocabulary with unique characters\n",
    "#\n",
    "# The first version of our vocabulary is simply all the unique characters\n",
    "# that appear in the training corpus.\n",
    "# Each character will be treated as an initial token.\n",
    "#\n",
    "# In addition, we add a special end-of-word marker (</w>).\n",
    "# This marker helps the tokenizer know where words end, so that\n",
    "# frequent whole words or subwords can be merged properly later.\n",
    "#\n",
    "# Example:\n",
    "#   \"this\"  →  [\"t\", \"h\", \"i\", \"s</w>\"]\n",
    "#   \"is\"    →  [\"i\", \"s</w>\"]\n",
    "#\n",
    "unique_chars = set()\n",
    "for doc in corpus:\n",
    "    for char in doc:\n",
    "        unique_chars.add(char)\n",
    "\n",
    "# Convert to a sorted list so the vocabulary is consistent and reproducible\n",
    "vocab = list(unique_chars)\n",
    "vocab.sort()\n",
    "\n",
    "# Add the special end-of-word token\n",
    "end_of_word = \"</w>\"\n",
    "vocab.append(end_of_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed80eff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Vocabulary:\n",
      "[' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>']\n",
      "Vocabulary Size: 20\n"
     ]
    }
   ],
   "source": [
    "print(\"Initial Vocabulary:\")\n",
    "print(vocab)\n",
    "print(f\"Vocabulary Size: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a386ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pre-tokenized Word Frequencies:\n",
      "{('T', 'h', 'i', 's', '</w>'): 2, ('i', 's', '</w>'): 3, ('t', 'h', 'e', '</w>'): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('t', 'h', 'i', 's', '</w>'): 2, ('t', 'h', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's', '</w>'): 1, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Pre-tokenize the corpus\n",
    "#\n",
    "# Goal:\n",
    "# - Split text into words (by spaces, for simplicity).\n",
    "# - Break each word into its characters.\n",
    "# - Add the special end-of-word token (</w>) at the end of every word.\n",
    "#\n",
    "# Why?\n",
    "# - This gives us the initial representation of words as sequences of characters.\n",
    "# - Example: \"This\" → (\"T\", \"h\", \"i\", \"s\", \"</w>\")\n",
    "#\n",
    "# Implementation details:\n",
    "# - We store each word as a tuple of characters (immutable).\n",
    "#   Tuples can be used as dictionary keys, unlike lists.\n",
    "# - We count how many times each word (as a sequence of characters) appears\n",
    "#   in the whole corpus.\n",
    "#\n",
    "# Note:\n",
    "# - Adding the </w> token ensures that subwords are learned within word\n",
    "#   boundaries. For example:\n",
    "#   \"document\" → (\"d\", \"o\", \"c\", \"u\", \"m\", \"e\", \"n\", \"t\", \"</w>\")\n",
    "#   This way, if \"doc\" becomes a frequent subword, it is clear that it\n",
    "#   belongs inside the word \"document\" and not across words.\n",
    "#\n",
    "word_splits = {}\n",
    "for doc in corpus:\n",
    "    words = doc.split(' ')\n",
    "    for word in words:\n",
    "        if word:\n",
    "            # Represent word as characters + </w>\n",
    "            char_list = list(word) + [end_of_word]\n",
    "            word_tuple = tuple(char_list)\n",
    "            \n",
    "            # Count frequency of this word form\n",
    "            if word_tuple not in word_splits:\n",
    "                word_splits[word_tuple] = 0\n",
    "            word_splits[word_tuple] += 1\n",
    "\n",
    "print(\"\\nPre-tokenized Word Frequencies:\")\n",
    "print(word_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ffba394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Count symbol pair frequencies\n",
    "#\n",
    "# Goal:\n",
    "# - Take the dictionary of word splits we created (word_splits).\n",
    "# - For each word, look at all adjacent pairs of symbols.\n",
    "# - Count how many times each pair appears across the entire corpus.\n",
    "#\n",
    "# Example:\n",
    "#   Input: {(\"T\", \"h\", \"i\", \"s\", \"</w>\"): 2}\n",
    "#   Output: {(\"T\", \"h\"): 2, (\"h\", \"i\"): 2, (\"i\", \"s\"): 2, (\"s\", \"</w>\"): 2}\n",
    "\n",
    "import collections \n",
    "\n",
    "def get_pair_stats(splits):\n",
    "    \"\"\"\n",
    "    Count the frequency of adjacent symbol pairs in the word_splits dictionary.\n",
    "    Example:\n",
    "        {('T', 'h', 'i', 's', '</w>'): 2, ('i', 's', '</w>'): 3, ...}\n",
    "    Output:\n",
    "        {('T', 'h'): 2, ('h', 'i'): 2, ('i', 's'): 5, ...}\n",
    "    \"\"\"\n",
    "\n",
    "    pair_counts = collections.defaultdict(int)\n",
    "    # there is a difference between the normal dict of python and collections.defaultdict \n",
    "    # the difference is: in a normal dict if we try to call a key that doesn't exist,\n",
    "    # it will throw an error. \n",
    "    # but this dictionary will create that key automatically,\n",
    "    # and the value it assigns to that key will be whatever we pass to the function \"int\",\n",
    "    # in this case it assigns zero.\n",
    "\n",
    "    for word_tuple, freq in splits.items():\n",
    "        # Example: ('T', 'h', 'i', 's', '</w>'): 2\n",
    "        # word_tuple = ('T', 'h', 'i', 's', '</w>')\n",
    "        # freq = 2 \n",
    "        symbols = list(word_tuple)\n",
    "        # symbols = ['T', 'h', 'i', 's', '</w>']\n",
    "        for i in range(len(symbols) - 1):\n",
    "            # len(symbols) - 1 ensures the last pair doesn't go out of range \n",
    "            # because we are accessing symbols[i+1]\n",
    "            pair = (symbols[i], symbols[i+1])\n",
    "            # Example: pair = ('T', 'h')\n",
    "            pair_counts[pair] += freq  # adding the frequency of the pair\n",
    "            # Example: pair_counts = {('T', 'h'): freq}\n",
    "    return pair_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6528485b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Merge the most frequent pair\n",
    "#\n",
    "# Goal:\n",
    "# - Take the most frequent pair of symbols (from pair_counts).\n",
    "# - Go through every word in word_splits.\n",
    "# - Replace occurrences of that pair with a new merged token.\n",
    "# - Keep track of merges (so we could undo or review them later if needed).\n",
    "#\n",
    "# Example:\n",
    "#   Input pair_to_merge: ('i', 's')\n",
    "#   Input word_splits: {\n",
    "#       ('T', 'h', 'i', 's', '</w>'): 2,\n",
    "#       ('i', 's', '</w>'): 3,\n",
    "#       ...\n",
    "#   }\n",
    "#   Output new_splits: {\n",
    "#       ('T', 'h', 'is', '</w>'): 2,\n",
    "#       ('is', '</w>'): 3,\n",
    "#       ...\n",
    "#   }\n",
    "#\n",
    "def merge_pair(pair_to_merge, splits):\n",
    "    \"\"\"Merges the specified pair in the word splits.\"\"\"\n",
    "    new_splits = {} # empty dictionary \n",
    "    (first, second) = pair_to_merge # ('i','s') → first = 'i', second = 's'\n",
    "    merged_token = first + second   # merged_token = 'is' (a string)\n",
    "\n",
    "    for word_tuple, freq in splits.items():\n",
    "        # Example: word_tuple = ('T', 'h', 'i', 's', '</w>'), freq = 2\n",
    "        symbols = list(word_tuple)\n",
    "        # symbols = ['T', 'h', 'i', 's', '</w>']\n",
    "        new_symbols = []\n",
    "        i = 0\n",
    "        while i < len(symbols):\n",
    "            if i < len(symbols) - 1 and symbols[i] == first and symbols[i+1] == second:\n",
    "                new_symbols.append(merged_token)\n",
    "                # if symbols[i] and symbols[i+1] match the pair_to_merge,\n",
    "                # replace them with the merged token\n",
    "                i += 2 \n",
    "            else:\n",
    "                # otherwise, just add the current character\n",
    "                new_symbols.append(symbols[i])\n",
    "                i += 1\n",
    "        # store the updated sequence of symbols as a tuple, with the same frequency\n",
    "        new_splits[tuple(new_symbols)] = freq\n",
    "    return new_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "704bbb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Quick recap of our data structures (with examples from our corpus)\n",
    "#\n",
    "# Corpus (toy example):\n",
    "# [\n",
    "#   \"This is the first document.\",\n",
    "#   \"This document is the second document.\",\n",
    "#   \"And this is the third one.\",\n",
    "#   \"Is this the first document?\",\n",
    "# ]\n",
    "#\n",
    "# 1) vocab\n",
    "#    - Definition: list of ALL unique characters found in the corpus, plus the end-of-word marker </w>.\n",
    "#    - Example (one possible order, after sorting and then appending </w>):\n",
    "#      [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>']\n",
    "#    - Purpose: this is our initial token set (characters).\n",
    "#\n",
    "# 2) word_splits\n",
    "#    - Definition: a dictionary mapping each WORD (as a tuple of characters + '</w>') to its frequency in the corpus.\n",
    "#      e.g., { ('T','h','i','s','</w>'): 2, ('i','s','</w>'): 3, ... }\n",
    "#    - How it’s built: split each line by spaces → take each word → turn into list(word) + ['</w>'] → tuple(...) as key.\n",
    "#    - Notes on punctuation and case:\n",
    "#        * Punctuation stays inside the word (e.g., \"document.\" becomes ('d','o','c','u','m','e','n','t','.', '</w>')).\n",
    "#        * Case is preserved (e.g., \"This\" vs \"this\" are different keys).\n",
    "#    - Concrete examples from our corpus:\n",
    "#        * ('T','h','i','s','</w>') appears 2 times  → \"This\" (line 1 and 2)\n",
    "#        * ('t','h','i','s','</w>') appears 2 times  → \"this\" (line 3 and 4)\n",
    "#        * ('i','s','</w>') appears 3 times         → \"is\"   (lines 1, 2, 3)\n",
    "#\n",
    "# 3) get_pair_state(splits)\n",
    "#    - Input: the word_splits dictionary.\n",
    "#    - Output: a dictionary that counts ADJACENT symbol pairs INSIDE each word (never across word boundaries).\n",
    "#      Example output shape: { ('T','h'): 2, ('h','i'): 4, ('i','s'): 7, ('s','</w>'): 7, ... }\n",
    "#    - How it works:\n",
    "#        * For each word tuple like ('T','h','i','s','</w>') with freq=2,\n",
    "#          it adds counts for ('T','h'), ('h','i'), ('i','s'), ('s','</w>') each +2.\n",
    "#        * It never considers pairs across words (e.g., it does NOT pair the last char of one word with the first of the next).\n",
    "#    - Concrete intuition with our corpus:\n",
    "#        * ('i','s') appears inside:\n",
    "#            - \"This\" (upper T) → 2 times total\n",
    "#            - \"this\" (lower t) → 2 times total\n",
    "#            - \"is\" as a word   → 3 times total (pairs: ('i','s') and ('s','</w>'))\n",
    "#          So ('i','s') can easily sum to 2 + 2 + 3 = 7 in the pair counts.\n",
    "#\n",
    "# 4) merge_pair(pair_to_merge, splits)\n",
    "#    - Input:\n",
    "#        * pair_to_merge: a tuple like ('i','s') — typically the MOST frequent pair from get_pair_state.\n",
    "#        * splits: the current word_splits dictionary.\n",
    "#    - Process:\n",
    "#        * For every word tuple, find every occurrence of the adjacent pair ('i','s') and replace it with the merged token \"is\".\n",
    "#        * Return a NEW dictionary (same shape as word_splits) but with updated word tuples that include the merged token.\n",
    "#    - Output shape (same as word_splits):\n",
    "#        * Before:\n",
    "#            {\n",
    "#              ('T','h','i','s','</w>'): 2,\n",
    "#              ('i','s','</w>'): 3,\n",
    "#              ...\n",
    "#            }\n",
    "#        * After merging ('i','s') → \"is\":\n",
    "#            {\n",
    "#              ('T','h','is','</w>'): 2,\n",
    "#              ('is','</w>'): 3,\n",
    "#              ...\n",
    "#            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a4a6eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting BPE Merges ---\n",
      "Initial Splits: {('T', 'h', 'i', 's', '</w>'): 2, ('i', 's', '</w>'): 3, ('t', 'h', 'e', '</w>'): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('t', 'h', 'i', 's', '</w>'): 2, ('t', 'h', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's', '</w>'): 1, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 1/15\n",
      "Top 5 Pair Frequencies: [(('s', '</w>'), 8), (('i', 's'), 7), (('t', 'h'), 7), (('h', 'i'), 5), (('h', 'e'), 4)]\n",
      "Found Best Pair: ('s', '</w>') with Frequency: 8\n",
      "Merging ('s', '</w>') into 's</w>'\n",
      "Splits after merge: {('T', 'h', 'i', 's</w>'): 2, ('i', 's</w>'): 3, ('t', 'h', 'e', '</w>'): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('t', 'h', 'i', 's</w>'): 2, ('t', 'h', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>']\n",
      "Updated Merges: {('s', '</w>'): 's</w>'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 2/15\n",
      "Top 5 Pair Frequencies: [(('i', 's</w>'), 7), (('t', 'h'), 7), (('h', 'i'), 5), (('h', 'e'), 4), (('e', '</w>'), 4)]\n",
      "Found Best Pair: ('i', 's</w>') with Frequency: 7\n",
      "Merging ('i', 's</w>') into 'is</w>'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('t', 'h', 'e', '</w>'): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('t', 'h', 'is</w>'): 2, ('t', 'h', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 3/15\n",
      "Top 5 Pair Frequencies: [(('t', 'h'), 7), (('h', 'is</w>'), 4), (('h', 'e'), 4), (('e', '</w>'), 4), (('d', 'o'), 4)]\n",
      "Found Best Pair: ('t', 'h') with Frequency: 7\n",
      "Merging ('t', 'h') into 'th'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('th', 'e', '</w>'): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 4/15\n",
      "Top 5 Pair Frequencies: [(('th', 'e'), 4), (('e', '</w>'), 4), (('d', 'o'), 4), (('o', 'c'), 4), (('c', 'u'), 4)]\n",
      "Found Best Pair: ('th', 'e') with Frequency: 4\n",
      "Merging ('th', 'e') into 'the'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the', '</w>'): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 5/15\n",
      "Top 5 Pair Frequencies: [(('the', '</w>'), 4), (('d', 'o'), 4), (('o', 'c'), 4), (('c', 'u'), 4), (('u', 'm'), 4)]\n",
      "Found Best Pair: ('the', '</w>') with Frequency: 4\n",
      "Merging ('the', '</w>') into 'the</w>'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 6/15\n",
      "Top 5 Pair Frequencies: [(('d', 'o'), 4), (('o', 'c'), 4), (('c', 'u'), 4), (('u', 'm'), 4), (('m', 'e'), 4)]\n",
      "Found Best Pair: ('d', 'o') with Frequency: 4\n",
      "Merging ('d', 'o') into 'do'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('do', 'c', 'u', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('do', 'c', 'u', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('do', 'c', 'u', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>', ('d', 'o'): 'do'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 7/15\n",
      "Top 5 Pair Frequencies: [(('do', 'c'), 4), (('c', 'u'), 4), (('u', 'm'), 4), (('m', 'e'), 4), (('e', 'n'), 4)]\n",
      "Found Best Pair: ('do', 'c') with Frequency: 4\n",
      "Merging ('do', 'c') into 'doc'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('doc', 'u', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('doc', 'u', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('doc', 'u', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>', ('d', 'o'): 'do', ('do', 'c'): 'doc'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 8/15\n",
      "Top 5 Pair Frequencies: [(('doc', 'u'), 4), (('u', 'm'), 4), (('m', 'e'), 4), (('e', 'n'), 4), (('n', 't'), 4)]\n",
      "Found Best Pair: ('doc', 'u') with Frequency: 4\n",
      "Merging ('doc', 'u') into 'docu'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('docu', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('docu', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('docu', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>', ('d', 'o'): 'do', ('do', 'c'): 'doc', ('doc', 'u'): 'docu'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 9/15\n",
      "Top 5 Pair Frequencies: [(('docu', 'm'), 4), (('m', 'e'), 4), (('e', 'n'), 4), (('n', 't'), 4), (('i', 'r'), 3)]\n",
      "Found Best Pair: ('docu', 'm') with Frequency: 4\n",
      "Merging ('docu', 'm') into 'docum'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('docum', 'e', 'n', 't', '.', '</w>'): 2, ('docum', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('docum', 'e', 'n', 't', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu', 'docum']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>', ('d', 'o'): 'do', ('do', 'c'): 'doc', ('doc', 'u'): 'docu', ('docu', 'm'): 'docum'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 10/15\n",
      "Top 5 Pair Frequencies: [(('docum', 'e'), 4), (('e', 'n'), 4), (('n', 't'), 4), (('i', 'r'), 3), (('t', '</w>'), 3)]\n",
      "Found Best Pair: ('docum', 'e') with Frequency: 4\n",
      "Merging ('docum', 'e') into 'docume'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('docume', 'n', 't', '.', '</w>'): 2, ('docume', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('docume', 'n', 't', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu', 'docum', 'docume']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>', ('d', 'o'): 'do', ('do', 'c'): 'doc', ('doc', 'u'): 'docu', ('docu', 'm'): 'docum', ('docum', 'e'): 'docume'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 11/15\n",
      "Top 5 Pair Frequencies: [(('docume', 'n'), 4), (('n', 't'), 4), (('i', 'r'), 3), (('t', '</w>'), 3), (('.', '</w>'), 3)]\n",
      "Found Best Pair: ('docume', 'n') with Frequency: 4\n",
      "Merging ('docume', 'n') into 'documen'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('documen', 't', '.', '</w>'): 2, ('documen', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('documen', 't', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu', 'docum', 'docume', 'documen']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>', ('d', 'o'): 'do', ('do', 'c'): 'doc', ('doc', 'u'): 'docu', ('docu', 'm'): 'docum', ('docum', 'e'): 'docume', ('docume', 'n'): 'documen'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 12/15\n",
      "Top 5 Pair Frequencies: [(('documen', 't'), 4), (('i', 'r'), 3), (('t', '</w>'), 3), (('.', '</w>'), 3), (('d', '</w>'), 3)]\n",
      "Found Best Pair: ('documen', 't') with Frequency: 4\n",
      "Merging ('documen', 't') into 'document'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('document', '.', '</w>'): 2, ('document', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('document', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu', 'docum', 'docume', 'documen', 'document']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>', ('d', 'o'): 'do', ('do', 'c'): 'doc', ('doc', 'u'): 'docu', ('docu', 'm'): 'docum', ('docum', 'e'): 'docume', ('docume', 'n'): 'documen', ('documen', 't'): 'document'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 13/15\n",
      "Top 5 Pair Frequencies: [(('i', 'r'), 3), (('.', '</w>'), 3), (('d', '</w>'), 3), (('T', 'h'), 2), (('h', 'is</w>'), 2)]\n",
      "Found Best Pair: ('i', 'r') with Frequency: 3\n",
      "Merging ('i', 'r') into 'ir'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'ir', 's', 't', '</w>'): 2, ('document', '.', '</w>'): 2, ('document', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'ir', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('document', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu', 'docum', 'docume', 'documen', 'document', 'ir']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>', ('d', 'o'): 'do', ('do', 'c'): 'doc', ('doc', 'u'): 'docu', ('docu', 'm'): 'docum', ('docum', 'e'): 'docume', ('docume', 'n'): 'documen', ('documen', 't'): 'document', ('i', 'r'): 'ir'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 14/15\n",
      "Top 5 Pair Frequencies: [(('.', '</w>'), 3), (('d', '</w>'), 3), (('T', 'h'), 2), (('h', 'is</w>'), 2), (('f', 'ir'), 2)]\n",
      "Found Best Pair: ('.', '</w>') with Frequency: 3\n",
      "Merging ('.', '</w>') into '.</w>'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'ir', 's', 't', '</w>'): 2, ('document', '.</w>'): 2, ('document', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'ir', 'd', '</w>'): 1, ('o', 'n', 'e', '.</w>'): 1, ('I', 's</w>'): 1, ('document', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu', 'docum', 'docume', 'documen', 'document', 'ir', '.</w>']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>', ('d', 'o'): 'do', ('do', 'c'): 'doc', ('doc', 'u'): 'docu', ('docu', 'm'): 'docum', ('docum', 'e'): 'docume', ('docume', 'n'): 'documen', ('documen', 't'): 'document', ('i', 'r'): 'ir', ('.', '</w>'): '.</w>'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 15/15\n",
      "Top 5 Pair Frequencies: [(('d', '</w>'), 3), (('T', 'h'), 2), (('h', 'is</w>'), 2), (('f', 'ir'), 2), (('ir', 's'), 2)]\n",
      "Found Best Pair: ('d', '</w>') with Frequency: 3\n",
      "Merging ('d', '</w>') into 'd</w>'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'ir', 's', 't', '</w>'): 2, ('document', '.</w>'): 2, ('document', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd</w>'): 1, ('A', 'n', 'd</w>'): 1, ('th', 'is</w>'): 2, ('th', 'ir', 'd</w>'): 1, ('o', 'n', 'e', '.</w>'): 1, ('I', 's</w>'): 1, ('document', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu', 'docum', 'docume', 'documen', 'document', 'ir', '.</w>', 'd</w>']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>', ('d', 'o'): 'do', ('do', 'c'): 'doc', ('doc', 'u'): 'docu', ('docu', 'm'): 'docum', ('docum', 'e'): 'docume', ('docume', 'n'): 'documen', ('documen', 't'): 'document', ('i', 'r'): 'ir', ('.', '</w>'): '.</w>', ('d', '</w>'): 'd</w>'}\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Iterative BPE merging loop\n",
    "#\n",
    "# Goal:\n",
    "# - Repeatedly find the most frequent adjacent symbol pair in the corpus (within words),\n",
    "#   merge that pair into a new token, and update our splits/vocabulary/merge rules.\n",
    "# - Do this up to `num_merges` times, or stop early if no more pairs exist.\n",
    "#\n",
    "# Key variables (with concrete examples from our toy corpus):\n",
    "# - num_merges: the maximum number of merge steps to perform (e.g., 15).\n",
    "# - merges: a dictionary storing learned merge rules. Example after a few steps:\n",
    "#       { ('i','s'): 'is', ('t','h'): 'th' }\n",
    "#   This means we learned to merge 'i' + 's' → \"is\", and 't' + 'h' → \"th\".\n",
    "# - current_splits: the working copy of word_splits that we keep updating after each merge.\n",
    "#   Example (before any merges):\n",
    "#       {\n",
    "#         ('T','h','i','s','</w>'): 2,\n",
    "#         ('i','s','</w>'): 3,\n",
    "#         ('t','h','e','</w>'): 4,\n",
    "#         ('d','o','c','u','m','e','n','t','.','</w>'): 3,\n",
    "#         ...\n",
    "#       }\n",
    "#\n",
    "# Functions used:\n",
    "# - get_pair_stats(splits) → dict:\n",
    "#       Input: current_splits (dict of word tuples → frequency).\n",
    "#       Output: dictionary mapping adjacent symbol pairs to total frequency in the corpus.\n",
    "#       Example:\n",
    "#           { ('i','s'): 7, ('s','</w>'): 8, ('t','h'): 7, ('h','i'): 5, ('T','h'): 2, ... }\n",
    "# - merge_pair(pair_to_merge, splits) → dict:\n",
    "#       Input: a pair like ('i','s') and current_splits.\n",
    "#       Process: replaces every occurrence of that pair inside each word tuple with the merged token (\"is\").\n",
    "#       Output: a new splits dict with updated word tuples.\n",
    "#       Example (effect on entries):\n",
    "#           Before: ('i','s','</w>') → After: ('is','</w>')\n",
    "#           Before: ('T','h','i','s','</w>') → After: ('T','h','is','</w>')\n",
    "#\n",
    "num_merges = 15\n",
    "\n",
    "# Stores merge rules, e.g., {('a','b'):'ab'}, {('T','h'):'Th'}\n",
    "merges = {}\n",
    "\n",
    "# Start from the initial word_splits (words → frequency) and update it iteratively.\n",
    "current_splits = word_splits.copy()\n",
    "\n",
    "print(\"\\n--- Starting BPE Merges ---\")\n",
    "print(f\"Initial Splits: {current_splits}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "for i in range(num_merges):\n",
    "    print(f\"\\nMerge Iteration {i+1}/{num_merges}\")\n",
    "\n",
    "    # 1) Calculate pair frequencies across all words in current_splits.\n",
    "    #    pair_stats is a dict mapping adjacent pairs → total frequency.\n",
    "    #    Example shape:\n",
    "    #      { ('T','h'): 2, ('h','i'): 5, ('i','s'): 7, ('s','</w>'): 8,\n",
    "    #        ('t','h'): 7, ('h','e'): 4, ('e','</w>'): 4, ('f','i'): 2, ... }\n",
    "    pair_stats = get_pair_stats(current_splits)\n",
    "\n",
    "    # If there are no adjacent pairs left (e.g., all words are single tokens), we stop.\n",
    "    if not pair_stats:\n",
    "        print(\"No more pairs to merge.\")\n",
    "        break\n",
    "\n",
    "    # 2) Inspect the top pairs (optional, for debugging/understanding).\n",
    "    #    Explanation:\n",
    "    #    - pair_stats.items() produces a list-like view of (pair, freq) tuples.\n",
    "    #    - key=lambda item: item[1] sorts by the frequency value.\n",
    "    #    - reverse=True sorts from highest to lowest frequency.\n",
    "    #    Result example:\n",
    "    #      [(('s','</w>'), 8), (('i','s'), 7), (('t','h'), 7), (('h','i'), 5), (('T','h'), 2)]\n",
    "    sorted_pairs = sorted(pair_stats.items(), key=lambda item: item[1], reverse=True)\n",
    "    print(f\"Top 5 Pair Frequencies: {sorted_pairs[:5]}\")\n",
    "\n",
    "    # 3) Select the single most frequent pair.\n",
    "    #    max will iterate over the keys of pair_stats; key=pair_stats.get compares by the dict value (frequency).\n",
    "    #    Example over { ('i','s'):7, ('s','</w>'):8, ('h','i'):5 } → best_pair = ('s','</w>')\n",
    "    best_pair = max(pair_stats, key=pair_stats.get)\n",
    "    best_freq = pair_stats[best_pair]  # (equivalently: pair_stats.get(best_pair))\n",
    "    print(f\"Found Best Pair: {best_pair} with Frequency: {best_freq}\")\n",
    "\n",
    "    # 4) Merge that best pair in every word where it appears.\n",
    "    #    Example:\n",
    "    #      best_pair = ('i','s') → new_token = \"is\"\n",
    "    #      Before: ('i','s','</w>')  → After: ('is','</w>')\n",
    "    #      Before: ('T','h','i','s','</w>') → After: ('T','h','is','</w>')\n",
    "    current_splits = merge_pair(best_pair, current_splits)\n",
    "    new_token = best_pair[0] + best_pair[1]\n",
    "    print(f\"Merging {best_pair} into '{new_token}'\")\n",
    "    print(f\"Splits after merge: {current_splits}\")\n",
    "\n",
    "    # 5) Update vocabulary: add the newly created token (e.g., \"is\", \"th\", \"doc\").\n",
    "    #    This grows our token set from characters → subwords → common words.\n",
    "    vocab.append(new_token)\n",
    "    print(f\"Updated Vocabulary: {vocab}\")\n",
    "\n",
    "    # 6) Record the merge rule so we can reproduce tokenization later on new text.\n",
    "    #    Example: merges[('i','s')] = 'is'\n",
    "    merges[best_pair] = new_token\n",
    "    print(f\"Updated Merges: {merges}\")\n",
    "    print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48812da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- BPE Merges Complete ---\n",
      "Final Vocabulary Size: 35\n",
      "\n",
      "Learned Merges (Pair -> New Token):\n",
      "('s', '</w>') -> 's</w>'\n",
      "('i', 's</w>') -> 'is</w>'\n",
      "('t', 'h') -> 'th'\n",
      "('th', 'e') -> 'the'\n",
      "('the', '</w>') -> 'the</w>'\n",
      "('d', 'o') -> 'do'\n",
      "('do', 'c') -> 'doc'\n",
      "('doc', 'u') -> 'docu'\n",
      "('docu', 'm') -> 'docum'\n",
      "('docum', 'e') -> 'docume'\n",
      "('docume', 'n') -> 'documen'\n",
      "('documen', 't') -> 'document'\n",
      "('i', 'r') -> 'ir'\n",
      "('.', '</w>') -> '.</w>'\n",
      "('d', '</w>') -> 'd</w>'\n",
      "\n",
      "Final Word Splits after all merges:\n",
      "{('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'ir', 's', 't', '</w>'): 2, ('document', '.</w>'): 2, ('document', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd</w>'): 1, ('A', 'n', 'd</w>'): 1, ('th', 'is</w>'): 2, ('th', 'ir', 'd</w>'): 1, ('o', 'n', 'e', '.</w>'): 1, ('I', 's</w>'): 1, ('document', '?', '</w>'): 1}\n",
      "\n",
      "Final Vocabulary (sorted):\n",
      "[' ', '.', '.</w>', '</w>', '?', 'A', 'I', 'T', 'c', 'd', 'd</w>', 'do', 'doc', 'docu', 'docum', 'docume', 'documen', 'document', 'e', 'f', 'h', 'i', 'ir', 'is</w>', 'm', 'n', 'o', 'r', 's', 's</w>', 't', 'th', 'the', 'the</w>', 'u']\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Review Final Results\n",
    "#\n",
    "#\n",
    "# Example of what to expect (simplified):\n",
    "# - merges:\n",
    "#     { ('i','s'):'is', ('t','h'):'th', ('Th','is'):'This', ... }\n",
    "# - current_splits:\n",
    "#     { ('This','</w>'):2, ('is','</w>'):3, ('doc','ument','.</w>'):3, ... }\n",
    "# - vocab:\n",
    "#     [' ', '.', '?', 'A', 'I', 'T', ..., 'is', 'th', 'This', 'doc', ...]\n",
    "#\n",
    "\n",
    "# --- BPE Merges Complete ---\n",
    "print(\"\\n--- BPE Merges Complete ---\")\n",
    "\n",
    "# Size of the final vocabulary\n",
    "print(f\"Final Vocabulary Size: {len(vocab)}\")\n",
    "\n",
    "# 1) Learned merge rules (pair → new token)\n",
    "print(\"\\nLearned Merges (Pair -> New Token):\")\n",
    "for pair, token in merges.items():\n",
    "    # Example: ('i','s') -> 'is'\n",
    "    print(f\"{pair} -> '{token}'\")\n",
    "\n",
    "# 2) Final representation of words (after all merges)\n",
    "print(\"\\nFinal Word Splits after all merges:\")\n",
    "# Example: ('This','</w>'):2, ('is','</w>'):3, ...\n",
    "print(current_splits)\n",
    "\n",
    "# 3) Final Vocabulary\n",
    "print(\"\\nFinal Vocabulary (sorted):\")\n",
    "# Convert to set in case duplicates slipped in, then sort for consistent view\n",
    "final_vocab_sorted = sorted(list(set(vocab)))\n",
    "print(final_vocab_sorted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8aa20479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note on real-world tokenizers:\n",
    "#\n",
    "# In practice, tokenizers do not work directly with characters or subword strings.\n",
    "# Instead, each element of the vocabulary is mapped to a unique integer index.\n",
    "#\n",
    "# Why?\n",
    "# - Numbers are much faster for a computer to process than strings.\n",
    "# - Looking up an index in a table is more efficient than comparing text symbols.\n",
    "# - Integers take less memory to store than characters or strings.\n",
    "#\n",
    "# Example of a vocabulary-to-index mapping:\n",
    "# {\n",
    "#     \"!\": 0,\n",
    "#     \"\\\"\": 1,\n",
    "#     \"#\": 2,\n",
    "#     \"$\": 3,\n",
    "#     \"%\": 4,\n",
    "#     \"&\": 5,\n",
    "#     \"'\": 6,\n",
    "#     \"(\": 7,\n",
    "#     \")\": 8,\n",
    "#     \"*\": 9,\n",
    "#     \"+\": 10,\n",
    "#     ...\n",
    "# }\n",
    "#\n",
    "# With this setup:\n",
    "# - Each token (character, subword, or word) is represented by an integer ID.\n",
    "# - Any input text can be quickly converted to a sequence of numbers.\n",
    "# - This sequence of numbers is what is actually fed into the embedding layer of an LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de29ab8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
