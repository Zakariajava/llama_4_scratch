{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce245952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM FROM SCRATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4524bec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a Byte Pair Encoding (BPE) Tokenizer from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfa67fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare training data\n",
    "# Note: here we are NOT training a neural network.\n",
    "# We are just creating a list of tokens from text.\n",
    "#\n",
    "# What we need: a text corpus.\n",
    "# The tokenizer will learn \"merge rules\" based on how often pairs of characters appear.\n",
    "#\n",
    "# Example:\n",
    "#   \"i\"   -> token 1\n",
    "#   \"s\"   -> token 2\n",
    "#   \"is\"  -> token 3\n",
    "#\n",
    "# How it works:\n",
    "# 1. Start with text split into individual characters.\n",
    "#    (every character is always in the vocabulary)\n",
    "# 2. Count how often character pairs occur.\n",
    "# 3. Merge the most frequent pairs into new tokens (subwords).\n",
    "# 4. Over time, the tokenizer builds a vocabulary that mixes\n",
    "#    single characters + useful subword tokens.\n",
    "#\n",
    "# Important:\n",
    "# - Characters always remain as fallback tokens.\n",
    "# - Subwords get priority when tokenizing, making it more efficient.\n",
    "#\n",
    "# Concretely:\n",
    "# - We scan the text and count how many times each pair of characters appears.\n",
    "# - For example, if the pair \"is\" appears very often, we create a new token for it.\n",
    "# - This reduces computation: instead of processing \"i\" and \"s\" separately,\n",
    "#   we can treat \"is\" as a single token.\n",
    "# - Note: \"i\" and \"s\" still remain in the vocabulary as individual tokens,\n",
    "#   but whenever the pair \"is\" exists, it takes priority over the single characters.\n",
    "#\n",
    "# Iterative merges:\n",
    "# - The process continues on top of previously created tokens.\n",
    "# - For example, if \"is\" was already merged into a token, and we notice \"his\"\n",
    "#   appears frequently, we merge \"h\" + \"is\" → \"his\".\n",
    "# - Next, if \"this\" is common, we merge \"t\" + \"his\" → \"this\".\n",
    "# - This way, the vocabulary gradually grows from characters → subwords → whole words,\n",
    "#   depending on frequency in the training text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17effa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our corpus of data\n",
    "corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "984a2fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: \n",
      "This is the first document.\n",
      "This document is the second document.\n",
      "And this is the third one.\n",
      "Is this the first document?\n"
     ]
    }
   ],
   "source": [
    "print(\"Corpus: \")\n",
    "for doc in corpus:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dca3915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Initialize vocabulary with unique characters\n",
    "#\n",
    "# The first version of our vocabulary is simply all the unique characters\n",
    "# that appear in the training corpus.\n",
    "# Each character will be treated as an initial token.\n",
    "#\n",
    "# In addition, we add a special end-of-word marker (</w>).\n",
    "# This marker helps the tokenizer know where words end, so that\n",
    "# frequent whole words or subwords can be merged properly later.\n",
    "#\n",
    "# Example:\n",
    "#   \"this\"  →  [\"t\", \"h\", \"i\", \"s</w>\"]\n",
    "#   \"is\"    →  [\"i\", \"s</w>\"]\n",
    "#\n",
    "unique_chars = set()\n",
    "for doc in corpus:\n",
    "    for char in doc:\n",
    "        unique_chars.add(char)\n",
    "\n",
    "# Convert to a sorted list so the vocabulary is consistent and reproducible\n",
    "vocab = list(unique_chars)\n",
    "vocab.sort()\n",
    "\n",
    "# Add the special end-of-word token\n",
    "end_of_word = \"</w>\"\n",
    "vocab.append(end_of_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed80eff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Vocabulary:\n",
      "[' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>']\n",
      "Vocabulary Size: 20\n"
     ]
    }
   ],
   "source": [
    "print(\"Initial Vocabulary:\")\n",
    "print(vocab)\n",
    "print(f\"Vocabulary Size: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a386ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pre-tokenized Word Frequencies:\n",
      "{('T', 'h', 'i', 's', '</w>'): 2, ('i', 's', '</w>'): 3, ('t', 'h', 'e', '</w>'): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('t', 'h', 'i', 's', '</w>'): 2, ('t', 'h', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's', '</w>'): 1, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Pre-tokenize the corpus\n",
    "#\n",
    "# Goal:\n",
    "# - Split text into words (by spaces, for simplicity).\n",
    "# - Break each word into its characters.\n",
    "# - Add the special end-of-word token (</w>) at the end of every word.\n",
    "#\n",
    "# Why?\n",
    "# - This gives us the initial representation of words as sequences of characters.\n",
    "# - Example: \"This\" → (\"T\", \"h\", \"i\", \"s\", \"</w>\")\n",
    "#\n",
    "# Implementation details:\n",
    "# - We store each word as a tuple of characters (immutable).\n",
    "#   Tuples can be used as dictionary keys, unlike lists.\n",
    "# - We count how many times each word (as a sequence of characters) appears\n",
    "#   in the whole corpus.\n",
    "#\n",
    "# Note:\n",
    "# - Adding the </w> token ensures that subwords are learned within word\n",
    "#   boundaries. For example:\n",
    "#   \"document\" → (\"d\", \"o\", \"c\", \"u\", \"m\", \"e\", \"n\", \"t\", \"</w>\")\n",
    "#   This way, if \"doc\" becomes a frequent subword, it is clear that it\n",
    "#   belongs inside the word \"document\" and not across words.\n",
    "#\n",
    "word_splits = {}\n",
    "for doc in corpus:\n",
    "    words = doc.split(' ')\n",
    "    for word in words:\n",
    "        if word:\n",
    "            # Represent word as characters + </w>\n",
    "            char_list = list(word) + [end_of_word]\n",
    "            word_tuple = tuple(char_list)\n",
    "            \n",
    "            # Count frequency of this word form\n",
    "            if word_tuple not in word_splits:\n",
    "                word_splits[word_tuple] = 0\n",
    "            word_splits[word_tuple] += 1\n",
    "\n",
    "print(\"\\nPre-tokenized Word Frequencies:\")\n",
    "print(word_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ffba394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Count symbol pair frequencies\n",
    "#\n",
    "# Goal:\n",
    "# - Take the dictionary of word splits we created (word_splits).\n",
    "# - For each word, look at all adjacent pairs of symbols.\n",
    "# - Count how many times each pair appears across the entire corpus.\n",
    "#\n",
    "# Example:\n",
    "#   Input: {(\"T\", \"h\", \"i\", \"s\", \"</w>\"): 2}\n",
    "#   Output: {(\"T\", \"h\"): 2, (\"h\", \"i\"): 2, (\"i\", \"s\"): 2, (\"s\", \"</w>\"): 2}\n",
    "\n",
    "import collections \n",
    "\n",
    "def get_pair_stats(splits):\n",
    "    \"\"\"\n",
    "    Count the frequency of adjacent symbol pairs in the word_splits dictionary.\n",
    "    Example:\n",
    "        {('T', 'h', 'i', 's', '</w>'): 2, ('i', 's', '</w>'): 3, ...}\n",
    "    Output:\n",
    "        {('T', 'h'): 2, ('h', 'i'): 2, ('i', 's'): 5, ...}\n",
    "    \"\"\"\n",
    "\n",
    "    pair_counts = collections.defaultdict(int)\n",
    "    # there is a difference between the normal dict of python and collections.defaultdict \n",
    "    # the difference is: in a normal dict if we try to call a key that doesn't exist,\n",
    "    # it will throw an error. \n",
    "    # but this dictionary will create that key automatically,\n",
    "    # and the value it assigns to that key will be whatever we pass to the function \"int\",\n",
    "    # in this case it assigns zero.\n",
    "\n",
    "    for word_tuple, freq in splits.items():\n",
    "        # Example: ('T', 'h', 'i', 's', '</w>'): 2\n",
    "        # word_tuple = ('T', 'h', 'i', 's', '</w>')\n",
    "        # freq = 2 \n",
    "        symbols = list(word_tuple)\n",
    "        # symbols = ['T', 'h', 'i', 's', '</w>']\n",
    "        for i in range(len(symbols) - 1):\n",
    "            # len(symbols) - 1 ensures the last pair doesn't go out of range \n",
    "            # because we are accessing symbols[i+1]\n",
    "            pair = (symbols[i], symbols[i+1])\n",
    "            # Example: pair = ('T', 'h')\n",
    "            pair_counts[pair] += freq  # adding the frequency of the pair\n",
    "            # Example: pair_counts = {('T', 'h'): freq}\n",
    "    return pair_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6528485b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Merge the most frequent pair\n",
    "#\n",
    "# Goal:\n",
    "# - Take the most frequent pair of symbols (from pair_counts).\n",
    "# - Go through every word in word_splits.\n",
    "# - Replace occurrences of that pair with a new merged token.\n",
    "# - Keep track of merges (so we could undo or review them later if needed).\n",
    "#\n",
    "# Example:\n",
    "#   Input pair_to_merge: ('i', 's')\n",
    "#   Input word_splits: {\n",
    "#       ('T', 'h', 'i', 's', '</w>'): 2,\n",
    "#       ('i', 's', '</w>'): 3,\n",
    "#       ...\n",
    "#   }\n",
    "#   Output new_splits: {\n",
    "#       ('T', 'h', 'is', '</w>'): 2,\n",
    "#       ('is', '</w>'): 3,\n",
    "#       ...\n",
    "#   }\n",
    "#\n",
    "def merge_pair(pair_to_merge, splits):\n",
    "    \"\"\"Merges the specified pair in the word splits.\"\"\"\n",
    "    new_splits = {} # empty dictionary \n",
    "    (first, second) = pair_to_merge # ('i','s') → first = 'i', second = 's'\n",
    "    merged_token = first + second   # merged_token = 'is' (a string)\n",
    "\n",
    "    for word_tuple, freq in splits.items():\n",
    "        # Example: word_tuple = ('T', 'h', 'i', 's', '</w>'), freq = 2\n",
    "        symbols = list(word_tuple)\n",
    "        # symbols = ['T', 'h', 'i', 's', '</w>']\n",
    "        new_symbols = []\n",
    "        i = 0\n",
    "        while i < len(symbols):\n",
    "            if i < len(symbols) - 1 and symbols[i] == first and symbols[i+1] == second:\n",
    "                new_symbols.append(merged_token)\n",
    "                # if symbols[i] and symbols[i+1] match the pair_to_merge,\n",
    "                # replace them with the merged token\n",
    "                i += 2 \n",
    "            else:\n",
    "                # otherwise, just add the current character\n",
    "                new_symbols.append(symbols[i])\n",
    "                i += 1\n",
    "        # store the updated sequence of symbols as a tuple, with the same frequency\n",
    "        new_splits[tuple(new_symbols)] = freq\n",
    "    return new_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "704bbb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Quick recap of our data structures (with examples from our corpus)\n",
    "#\n",
    "# Corpus (toy example):\n",
    "# [\n",
    "#   \"This is the first document.\",\n",
    "#   \"This document is the second document.\",\n",
    "#   \"And this is the third one.\",\n",
    "#   \"Is this the first document?\",\n",
    "# ]\n",
    "#\n",
    "# 1) vocab\n",
    "#    - Definition: list of ALL unique characters found in the corpus, plus the end-of-word marker </w>.\n",
    "#    - Example (one possible order, after sorting and then appending </w>):\n",
    "#      [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>']\n",
    "#    - Purpose: this is our initial token set (characters).\n",
    "#\n",
    "# 2) word_splits\n",
    "#    - Definition: a dictionary mapping each WORD (as a tuple of characters + '</w>') to its frequency in the corpus.\n",
    "#      e.g., { ('T','h','i','s','</w>'): 2, ('i','s','</w>'): 3, ... }\n",
    "#    - How it’s built: split each line by spaces → take each word → turn into list(word) + ['</w>'] → tuple(...) as key.\n",
    "#    - Notes on punctuation and case:\n",
    "#        * Punctuation stays inside the word (e.g., \"document.\" becomes ('d','o','c','u','m','e','n','t','.', '</w>')).\n",
    "#        * Case is preserved (e.g., \"This\" vs \"this\" are different keys).\n",
    "#    - Concrete examples from our corpus:\n",
    "#        * ('T','h','i','s','</w>') appears 2 times  → \"This\" (line 1 and 2)\n",
    "#        * ('t','h','i','s','</w>') appears 2 times  → \"this\" (line 3 and 4)\n",
    "#        * ('i','s','</w>') appears 3 times         → \"is\"   (lines 1, 2, 3)\n",
    "#\n",
    "# 3) get_pair_state(splits)\n",
    "#    - Input: the word_splits dictionary.\n",
    "#    - Output: a dictionary that counts ADJACENT symbol pairs INSIDE each word (never across word boundaries).\n",
    "#      Example output shape: { ('T','h'): 2, ('h','i'): 4, ('i','s'): 7, ('s','</w>'): 7, ... }\n",
    "#    - How it works:\n",
    "#        * For each word tuple like ('T','h','i','s','</w>') with freq=2,\n",
    "#          it adds counts for ('T','h'), ('h','i'), ('i','s'), ('s','</w>') each +2.\n",
    "#        * It never considers pairs across words (e.g., it does NOT pair the last char of one word with the first of the next).\n",
    "#    - Concrete intuition with our corpus:\n",
    "#        * ('i','s') appears inside:\n",
    "#            - \"This\" (upper T) → 2 times total\n",
    "#            - \"this\" (lower t) → 2 times total\n",
    "#            - \"is\" as a word   → 3 times total (pairs: ('i','s') and ('s','</w>'))\n",
    "#          So ('i','s') can easily sum to 2 + 2 + 3 = 7 in the pair counts.\n",
    "#\n",
    "# 4) merge_pair(pair_to_merge, splits)\n",
    "#    - Input:\n",
    "#        * pair_to_merge: a tuple like ('i','s') — typically the MOST frequent pair from get_pair_state.\n",
    "#        * splits: the current word_splits dictionary.\n",
    "#    - Process:\n",
    "#        * For every word tuple, find every occurrence of the adjacent pair ('i','s') and replace it with the merged token \"is\".\n",
    "#        * Return a NEW dictionary (same shape as word_splits) but with updated word tuples that include the merged token.\n",
    "#    - Output shape (same as word_splits):\n",
    "#        * Before:\n",
    "#            {\n",
    "#              ('T','h','i','s','</w>'): 2,\n",
    "#              ('i','s','</w>'): 3,\n",
    "#              ...\n",
    "#            }\n",
    "#        * After merging ('i','s') → \"is\":\n",
    "#            {\n",
    "#              ('T','h','is','</w>'): 2,\n",
    "#              ('is','</w>'): 3,\n",
    "#              ...\n",
    "#            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4a6eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting BPE Merges ---\n",
      "Initial Splits: {('T', 'h', 'i', 's', '</w>'): 2, ('i', 's', '</w>'): 3, ('t', 'h', 'e', '</w>'): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('t', 'h', 'i', 's', '</w>'): 2, ('t', 'h', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's', '</w>'): 1, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 1/15\n",
      "\n",
      "Merge Iteration 2/15\n",
      "\n",
      "Merge Iteration 3/15\n",
      "\n",
      "Merge Iteration 4/15\n",
      "\n",
      "Merge Iteration 5/15\n",
      "\n",
      "Merge Iteration 6/15\n",
      "\n",
      "Merge Iteration 7/15\n",
      "\n",
      "Merge Iteration 8/15\n",
      "\n",
      "Merge Iteration 9/15\n",
      "\n",
      "Merge Iteration 10/15\n",
      "\n",
      "Merge Iteration 11/15\n",
      "\n",
      "Merge Iteration 12/15\n",
      "\n",
      "Merge Iteration 13/15\n",
      "\n",
      "Merge Iteration 14/15\n",
      "\n",
      "Merge Iteration 15/15\n"
     ]
    }
   ],
   "source": [
    "# Step 7: iterative BPE Merging loop\n",
    "\n",
    "num_merges = 15\n",
    "# Stores merge rules, e.g., {('a', 'b'): 'ab'} {('T', 'h'): 'Th'}\n",
    "merges = {}\n",
    "current_splits = word_splits.copy() # Start with initial word splits {('T', 'h', 'i', 's', '</w>'): 2, ('i', 's', '</w>'): 3, ('t', 'h', 'e', '</w>'): 4, ('f', 'i', 'r', \n",
    "\n",
    "print(\"\\n--- Starting BPE Merges ---\")\n",
    "print(f\"Initial Splits: {current_splits}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "for i in range(num_merges):\n",
    "    print(f\"\\nMerge Iteration {i+1}/{num_merges}\")\n",
    "    # 1. Calculate Pair Frequencies\n",
    "    pair_stats = get_pair_stats(current_splits) # pair_stats  {('T', 'h'): 2, ('h', 'i'): 5, ('i', 's'): 7, ('s', '</w>'): 8, ('t', 'h'): 7, ('h', 'e'): 4, ('e', '</w>'): 4, ('f', 'i'): 2,\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
