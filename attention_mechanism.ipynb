{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "300c4e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7c07337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Number of GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA available:\", torch.cuda.is_available())\n",
    "    print(\"Number of GPUs:\", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ba335b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration --- # \n",
    "\n",
    "hidden_size = 128\n",
    "# Dimension of the embedding vector for each token.\n",
    "# Example: every token (like \"dog\") is represented by a vector of 128 numbers.\n",
    "\n",
    "num_attention_heads = 16\n",
    "# How many attention heads we use in parallel.\n",
    "# The hidden_size (128) is split across these heads.\n",
    "# Each head sees only part of the vector → 128 / 16 = 8 dimensions per head.\n",
    "\n",
    "num_key_value_heads = 4\n",
    "# Special trick: Grouped-Query Attention (GQA).\n",
    "# Instead of creating 16 different Key/Value heads,\n",
    "# we only create 4 K/V heads (each of size 8) and let multiple Q heads share them.\n",
    "# → This saves memory and computation while keeping good performance.\n",
    "\n",
    "head_dim = hidden_size // num_attention_heads\n",
    "# Size of each head’s Q, K, V vector.\n",
    "# With hidden_size=128 and 16 heads, each head works in 8 dimensions.\n",
    "\n",
    "max_position_embeddings = 256\n",
    "# Maximum sequence length (number of tokens) the model can process at once.\n",
    "# If a sentence has more than 256 tokens, it must be truncated or split.\n",
    "\n",
    "rope_theta = 10000.0\n",
    "# Base frequency for Rotary Position Embeddings (RoPE).\n",
    "# It controls how positional information is encoded.\n",
    "# Larger theta = slower change in frequency = smoother positional encoding.\n",
    "\n",
    "rms_norm_eps = 1e-5\n",
    "# Tiny constant added inside RMSNorm to avoid division by zero.\n",
    "# Ensures stability in training and inference.\n",
    "\n",
    "attention_bias = False\n",
    "# Whether to add a bias term to the linear layers that produce Q, K, V.\n",
    "# Usually kept False for efficiency.\n",
    "\n",
    "attention_dropout = 0.0\n",
    "# Dropout probability applied to attention weights (to prevent overfitting).\n",
    "# Often set to 0.0 during inference (disabled).\n",
    "\n",
    "use_qk_norm = True\n",
    "# Whether to normalize Q and K vectors (L2 norm) before computing attention scores.\n",
    "# This keeps dot products more stable and avoids extreme attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5071a1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  hidden_size: 128\n",
      "  num_attention_heads: 16\n",
      "  num_key_value_heads: 4\n",
      "  head_dim: 8\n",
      "\n",
      "Sample Input Shapes:\n",
      "  hidden_states: torch.Size([2, 10, 128])\n",
      "  position_ids: torch.Size([2, 10])\n",
      "  attention_mask: torch.Size([2, 1, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "# --- Sample input setup ---\n",
    "\n",
    "batch_size = 2  \n",
    "# Number of independent sequences (context windows) processed in parallel.\n",
    "# Example: 2 separate sentences.\n",
    "\n",
    "sequence_length = 10  \n",
    "# Number of tokens in each sequence (the length of the context window).\n",
    "\n",
    "hidden_states = torch.randn(batch_size, sequence_length, hidden_size)  \n",
    "# Random embeddings for each token in each sequence.\n",
    "# Shape = (batch_size, sequence_length, hidden_size)\n",
    "#        = (2, 10, 128)\n",
    "# Meaning:\n",
    "# - 2 sequences\n",
    "# - Each sequence has 10 tokens\n",
    "# - Each token is represented by a 128-dimensional vector\n",
    "\n",
    "# --- Position IDs creation ---\n",
    "\n",
    "position_ids = torch.arange(0, sequence_length).unsqueeze(0).repeat(batch_size, 1)\n",
    "# position_ids = torch.arange(0, sequence_length)  \n",
    "# → Shape: (10,)\n",
    "# → [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# position_ids = position_ids.unsqueeze(0)  \n",
    "# Add a new dimension at the front\n",
    "# → Shape: (1, 10)\n",
    "# → [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]\n",
    "\n",
    "# position_ids = position_ids.repeat(batch_size, 1)  \n",
    "# Repeat the row for each sequence in the batch\n",
    "# → Shape: (2, 10)\n",
    "# → [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "#    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]\n",
    "\n",
    "# Intuition:\n",
    "# - Each token in each sequence needs a position ID.\n",
    "# - Both sequences start at position 0, because they are independent windows.\n",
    "\n",
    "# Create a causal attention mask\n",
    "# Goal: make sure each token can only see itself and tokens before it (no looking into the future)\n",
    "\n",
    "attention_mask = torch.triu(torch.ones(sequence_length, sequence_length) * -torch.inf, diagonal=1)\n",
    "# Step 1: Make a square matrix (seq x seq).\n",
    "# -∞ above the diagonal = future tokens (blocked)\n",
    "# 0 on and below diagonal = current/past tokens (allowed)\n",
    "\n",
    "attention_mask = attention_mask.unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, seq, seq)\n",
    "# Step 2: Add two extra dimensions so the mask matches attention shapes.\n",
    "# Now we have 4D: [1, 1, seq, seq]\n",
    "\n",
    "attention_mask = attention_mask.expand(batch_size, 1, -1, -1)  # Shape: (batch, 1, seq, seq)\n",
    "# Step 3: Copy the mask for each sequence in the batch.\n",
    "# The \"1\" in heads dimension means the same mask is shared across all attention heads.\n",
    "print(\"Configuration:\")\n",
    "print(f\"  hidden_size: {hidden_size}\")\n",
    "print(f\"  num_attention_heads: {num_attention_heads}\")\n",
    "print(f\"  num_key_value_heads: {num_key_value_heads}\")\n",
    "print(f\"  head_dim: {head_dim}\")\n",
    "\n",
    "print(\"\\nSample Input Shapes:\")\n",
    "print(f\"  hidden_states: {hidden_states.shape}\")\n",
    "print(f\"  position_ids: {position_ids.shape}\")\n",
    "print(f\"  attention_mask: {attention_mask.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4eb2aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projected Shapes:\n",
      "  query_states: torch.Size([2, 16, 10, 8])\n",
      "  key_states: torch.Size([2, 4, 10, 8])\n",
      "  value_states: torch.Size([2, 4, 10, 8])\n",
      "\n",
      "Num Key/Value Groups (Q heads per K/V head): 4\n"
     ]
    }
   ],
   "source": [
    "# ## Q, K, V Projections\n",
    "#\n",
    "# The first step of attention: project the input hidden states into\n",
    "# Query (Q), Key (K), and Value (V) spaces using linear layers.\n",
    "#\n",
    "# - Q = \"what am I looking for?\" (the current token’s query)\n",
    "# - K = \"what can I offer?\" (the key of each token in the sequence)\n",
    "# - V = \"what information do I carry?\" (the value of each token)\n",
    "#\n",
    "# In Llama (and many modern transformers) uses GQA = Grouped-Query Attention:\n",
    "# - There are more Q heads (16 here) than K/V heads (4 here).\n",
    "# - Multiple Q heads share the same K/V heads.\n",
    "# - This reduces memory/computation without losing much performance.\n",
    "\n",
    "\n",
    "# --- Define linear projection layers ---\n",
    "# Each Linear layer creates a weight matrix W and (optionally) a bias vector b.\n",
    "# PyTorch stores W with shape (out_features, in_features).\n",
    "# During forward, the computation is:  output = input @ W.T + b\n",
    "\n",
    "# Q projection: from hidden_size=128 → num_attention_heads * head_dim = 16*8=128\n",
    "# So shape of Wq is (128, 128). Each token gets projected into 16 Q-heads (each of size 8).\n",
    "q_proj = nn.Linear(hidden_size, num_attention_heads * head_dim, bias=attention_bias)\n",
    "\n",
    "# K projection: from hidden_size=128 → num_key_value_heads * head_dim = 4*8=32\n",
    "# So shape of Wk is (32, 128). Each token gets projected into 4 K-heads (each of size 8).\n",
    "k_proj = nn.Linear(hidden_size, num_key_value_heads * head_dim, bias=attention_bias)\n",
    "\n",
    "# V projection: same as K (4 heads of size 8).\n",
    "v_proj = nn.Linear(hidden_size, num_key_value_heads * head_dim, bias=attention_bias)\n",
    "\n",
    "# O projection: after attention, we concatenate the 16 Q-head outputs back into\n",
    "# a single vector (size 128). o_proj maps it back into hidden_size=128.\n",
    "o_proj = nn.Linear(num_attention_heads * head_dim, hidden_size, bias=attention_bias)\n",
    "\n",
    "\n",
    "# --- Apply projections to the hidden states ---\n",
    "# hidden_states has shape [batch_size, seq_len, hidden_size]\n",
    "# Each linear layer is applied independently to every token in the batch.\n",
    "\n",
    "# Q projection: output shape [batch_size, seq_len, num_attention_heads*head_dim] = [B, S, 128]\n",
    "query_states = q_proj(hidden_states)\n",
    "\n",
    "# K projection: output shape [batch_size, seq_len, num_key_value_heads*head_dim] = [B, S, 32]\n",
    "key_states = k_proj(hidden_states)\n",
    "\n",
    "# V projection: output shape [batch_size, seq_len, num_key_value_heads*head_dim] = [B, S, 32]\n",
    "value_states = v_proj(hidden_states)\n",
    "\n",
    "# Reshape queries, keys, values into [B, num_heads, S, head_dim] for multi-head attention\n",
    "# --------------------------------------------------------------\n",
    "# Before view: \n",
    "#   query_states shape = [B, S, hidden_size] = [1, 3, 4]\n",
    "#   (for each token we just have 4 numbers, flattened)\n",
    "#\n",
    "# Step 1 (view): cut hidden_size=4 into (num_heads=2, head_dim=2)\n",
    "#   query_states.view(1, 3, 2, 2) → [1, 3, 2, 2]\n",
    "#\n",
    "#   For token 0: [q0, q1 | q2, q3]   → head0=[q0,q1], head1=[q2,q3]\n",
    "#   For token 1: [q4, q5 | q6, q7]   → head0=[q4,q5], head1=[q6,q7]\n",
    "#   For token 2: [q8, q9 | q10,q11]  → head0=[q8,q9], head1=[q10,q11]\n",
    "#\n",
    "# Step 2 (transpose): put heads dimension before sequence length\n",
    "#   query_states.transpose(1, 2) → [1, 2, 3, 2]\n",
    "#\n",
    "#   Now we can think like:\n",
    "#     for b in batch:        # here b=0\n",
    "#       for h in heads:      # h=0..1\n",
    "#         query_states[b,h,:,:] = all tokens for this head\n",
    "#\n",
    "#   Example:\n",
    "#     query_states[0,0,:,:] = [[q0,q1], [q4,q5], [q8,q9]]  # head 0 across all tokens\n",
    "#     query_states[0,1,:,:] = [[q2,q3], [q6,q7], [q10,q11]]# head 1 across all tokens\n",
    "#\n",
    "# Keys and values go through the same reshape+transpose,\n",
    "# but with num_key_value_heads=4 in Llama (instead of 16 for queries).\n",
    "# So shapes end up:\n",
    "#   Q: [B, num_heads, S, head_dim]\n",
    "#   K: [B, num_kv_heads, S, head_dim]\n",
    "#   V: [B, num_kv_heads, S, head_dim]\n",
    "query_states = query_states.view(batch_size, sequence_length, num_attention_heads, head_dim).transpose(1, 2)\n",
    "key_states = key_states.view(batch_size, sequence_length, num_key_value_heads, head_dim).transpose(1, 2)\n",
    "value_states = value_states.view(batch_size, sequence_length, num_key_value_heads, head_dim).transpose(1, 2)\n",
    "\n",
    "print(\"Projected Shapes:\")\n",
    "print(f\"  query_states: {query_states.shape}\") # (batch_size, num_attention_heads, sequence_length, head_dim)\n",
    "print(f\"  key_states: {key_states.shape}\")     # (batch_size, num_key_value_heads, sequence_length, head_dim)\n",
    "print(f\"  value_states: {value_states.shape}\")   # (batch_size, num_key_value_heads, sequence_length, head_dim)\n",
    "\n",
    "num_key_value_groups = num_attention_heads // num_key_value_heads\n",
    "print(f\"\\nNum Key/Value Groups (Q heads per K/V head): {num_key_value_groups}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
