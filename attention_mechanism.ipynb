{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "300c4e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7c07337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Number of GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA available:\", torch.cuda.is_available())\n",
    "    print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ba335b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration --- # \n",
    "\n",
    "hidden_size = 128\n",
    "# Dimension of the embedding vector for each token.\n",
    "# Example: every token (like \"dog\") is represented by a vector of 128 numbers.\n",
    "\n",
    "num_attention_heads = 16\n",
    "# How many attention heads we use in parallel.\n",
    "# The hidden_size (128) is split across these heads.\n",
    "# Each head sees only part of the vector → 128 / 16 = 8 dimensions per head.\n",
    "\n",
    "num_key_value_heads = 4\n",
    "# Special trick: Grouped-Query Attention (GQA).\n",
    "# Instead of creating 16 different Key/Value heads,\n",
    "# we only create 4 K/V heads (each of size 8) and let multiple Q heads share them.\n",
    "# → This saves memory and computation while keeping good performance.\n",
    "\n",
    "head_dim = hidden_size // num_attention_heads\n",
    "# Size of each head’s Q, K, V vector.\n",
    "# With hidden_size=128 and 16 heads, each head works in 8 dimensions.\n",
    "\n",
    "max_position_embeddings = 256\n",
    "# Maximum sequence length (number of tokens) the model can process at once.\n",
    "# If a sentence has more than 256 tokens, it must be truncated or split.\n",
    "\n",
    "rope_theta = 10000.0\n",
    "# Base frequency for Rotary Position Embeddings (RoPE).\n",
    "# It controls how positional information is encoded.\n",
    "# Larger theta = slower change in frequency = smoother positional encoding.\n",
    "\n",
    "rms_norm_eps = 1e-5\n",
    "# Tiny constant added inside RMSNorm to avoid division by zero.\n",
    "# Ensures stability in training and inference.\n",
    "\n",
    "attention_bias = False\n",
    "# Whether to add a bias term to the linear layers that produce Q, K, V.\n",
    "# Usually kept False for efficiency.\n",
    "\n",
    "attention_dropout = 0.0\n",
    "# Dropout probability applied to attention weights (to prevent overfitting).\n",
    "# Often set to 0.0 during inference (disabled).\n",
    "\n",
    "use_qk_norm = True\n",
    "# Whether to normalize Q and K vectors (L2 norm) before computing attention scores.\n",
    "# This keeps dot products more stable and avoids extreme attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5071a1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  hidden_size: 128\n",
      "  num_attention_heads: 16\n",
      "  num_key_value_heads: 4\n",
      "  head_dim: 8\n",
      "\n",
      "Sample Input Shapes:\n",
      "  hidden_states: torch.Size([2, 10, 128])\n",
      "  position_ids: torch.Size([2, 10])\n",
      "  attention_mask: torch.Size([2, 1, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "# --- Sample input setup ---\n",
    "\n",
    "batch_size = 2  \n",
    "# Number of independent sequences (context windows) processed in parallel.\n",
    "# Example: 2 separate sentences.\n",
    "\n",
    "sequence_length = 10  \n",
    "# Number of tokens in each sequence (the length of the context window).\n",
    "\n",
    "hidden_states = torch.randn(batch_size, sequence_length, hidden_size)  \n",
    "# Random embeddings for each token in each sequence.\n",
    "# Shape = (batch_size, sequence_length, hidden_size)\n",
    "#        = (2, 10, 128)\n",
    "# Meaning:\n",
    "# - 2 sequences\n",
    "# - Each sequence has 10 tokens\n",
    "# - Each token is represented by a 128-dimensional vector\n",
    "\n",
    "# --- Position IDs creation ---\n",
    "\n",
    "position_ids = torch.arange(0, sequence_length).unsqueeze(0).repeat(batch_size, 1)\n",
    "# position_ids = torch.arange(0, sequence_length)  \n",
    "# → Shape: (10,)\n",
    "# → [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# position_ids = position_ids.unsqueeze(0)  \n",
    "# Add a new dimension at the front\n",
    "# → Shape: (1, 10)\n",
    "# → [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]\n",
    "\n",
    "# position_ids = position_ids.repeat(batch_size, 1)  \n",
    "# Repeat the row for each sequence in the batch\n",
    "# → Shape: (2, 10)\n",
    "# → [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "#    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]\n",
    "\n",
    "# Intuition:\n",
    "# - Each token in each sequence needs a position ID.\n",
    "# - Both sequences start at position 0, because they are independent windows.\n",
    "\n",
    "# Create a causal attention mask\n",
    "# Goal: make sure each token can only see itself and tokens before it (no looking into the future)\n",
    "\n",
    "attention_mask = torch.triu(torch.ones(sequence_length, sequence_length) * -torch.inf, diagonal=1)\n",
    "# Step 1: Make a square matrix (seq x seq).\n",
    "# -∞ above the diagonal = future tokens (blocked)\n",
    "# 0 on and below diagonal = current/past tokens (allowed)\n",
    "\n",
    "attention_mask = attention_mask.unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, seq, seq)\n",
    "# Step 2: Add two extra dimensions so the mask matches attention shapes.\n",
    "# Now we have 4D: [1, 1, seq, seq]\n",
    "\n",
    "attention_mask = attention_mask.expand(batch_size, 1, -1, -1)  # Shape: (batch, 1, seq, seq)\n",
    "# Step 3: Copy the mask for each sequence in the batch.\n",
    "# The \"1\" in heads dimension means the same mask is shared across all attention heads.\n",
    "print(\"Configuration:\")\n",
    "print(f\"  hidden_size: {hidden_size}\")\n",
    "print(f\"  num_attention_heads: {num_attention_heads}\")\n",
    "print(f\"  num_key_value_heads: {num_key_value_heads}\")\n",
    "print(f\"  head_dim: {head_dim}\")\n",
    "\n",
    "print(\"\\nSample Input Shapes:\")\n",
    "print(f\"  hidden_states: {hidden_states.shape}\")\n",
    "print(f\"  position_ids: {position_ids.shape}\")\n",
    "print(f\"  attention_mask: {attention_mask.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4eb2aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projected Shapes:\n",
      "  query_states: torch.Size([2, 16, 10, 8])\n",
      "  key_states: torch.Size([2, 4, 10, 8])\n",
      "  value_states: torch.Size([2, 4, 10, 8])\n",
      "\n",
      "Num Key/Value Groups (Q heads per K/V head): 4\n"
     ]
    }
   ],
   "source": [
    "# ## Q, K, V Projections\n",
    "#\n",
    "# The first step of attention: project the input hidden states into\n",
    "# Query (Q), Key (K), and Value (V) spaces using linear layers.\n",
    "#\n",
    "# - Q = \"what am I looking for?\" (the current token’s query)\n",
    "# - K = \"what can I offer?\" (the key of each token in the sequence)\n",
    "# - V = \"what information do I carry?\" (the value of each token)\n",
    "#\n",
    "# In Llama (and many modern transformers) uses GQA = Grouped-Query Attention:\n",
    "# - There are more Q heads (16 here) than K/V heads (4 here).\n",
    "# - Multiple Q heads share the same K/V heads.\n",
    "# - This reduces memory/computation without losing much performance.\n",
    "\n",
    "\n",
    "# --- Define linear projection layers ---\n",
    "# Each Linear layer creates a weight matrix W and (optionally) a bias vector b.\n",
    "# PyTorch stores W with shape (out_features, in_features).\n",
    "# During forward, the computation is:  output = input @ W.T + b\n",
    "\n",
    "# Q projection: from hidden_size=128 → num_attention_heads * head_dim = 16*8=128\n",
    "# So shape of Wq is (128, 128). Each token gets projected into 16 Q-heads (each of size 8).\n",
    "q_proj = nn.Linear(hidden_size, num_attention_heads * head_dim, bias=attention_bias)\n",
    "\n",
    "# K projection: from hidden_size=128 → num_key_value_heads * head_dim = 4*8=32\n",
    "# So shape of Wk is (32, 128). Each token gets projected into 4 K-heads (each of size 8).\n",
    "k_proj = nn.Linear(hidden_size, num_key_value_heads * head_dim, bias=attention_bias)\n",
    "\n",
    "# V projection: same as K (4 heads of size 8).\n",
    "v_proj = nn.Linear(hidden_size, num_key_value_heads * head_dim, bias=attention_bias)\n",
    "\n",
    "# O projection: after attention, we concatenate the 16 Q-head outputs back into\n",
    "# a single vector (size 128). o_proj maps it back into hidden_size=128.\n",
    "o_proj = nn.Linear(num_attention_heads * head_dim, hidden_size, bias=attention_bias)\n",
    "\n",
    "\n",
    "# --- Apply projections to the hidden states ---\n",
    "# hidden_states has shape [batch_size, seq_len, hidden_size]\n",
    "# Each linear layer is applied independently to every token in the batch.\n",
    "\n",
    "# Q projection: output shape [batch_size, seq_len, num_attention_heads*head_dim] = [B, S, 128]\n",
    "query_states = q_proj(hidden_states)\n",
    "\n",
    "# K projection: output shape [batch_size, seq_len, num_key_value_heads*head_dim] = [B, S, 32]\n",
    "key_states = k_proj(hidden_states)\n",
    "\n",
    "# V projection: output shape [batch_size, seq_len, num_key_value_heads*head_dim] = [B, S, 32]\n",
    "value_states = v_proj(hidden_states)\n",
    "\n",
    "# Reshape queries, keys, values into [B, num_heads, S, head_dim] for multi-head attention\n",
    "# --------------------------------------------------------------\n",
    "# Before view: \n",
    "#   query_states shape = [B, S, hidden_size] = [1, 3, 4]\n",
    "#   (for each token we just have 4 numbers, flattened)\n",
    "#\n",
    "# Step 1 (view): cut hidden_size=4 into (num_heads=2, head_dim=2)\n",
    "#   query_states.view(1, 3, 2, 2) → [1, 3, 2, 2]\n",
    "#\n",
    "#   For token 0: [q0, q1 | q2, q3]   → head0=[q0,q1], head1=[q2,q3]\n",
    "#   For token 1: [q4, q5 | q6, q7]   → head0=[q4,q5], head1=[q6,q7]\n",
    "#   For token 2: [q8, q9 | q10,q11]  → head0=[q8,q9], head1=[q10,q11]\n",
    "#\n",
    "# Step 2 (transpose): put heads dimension before sequence length\n",
    "#   query_states.transpose(1, 2) → [1, 2, 3, 2]\n",
    "#\n",
    "#   Now we can think like:\n",
    "#     for b in batch:        # here b=0\n",
    "#       for h in heads:      # h=0..1\n",
    "#         query_states[b,h,:,:] = all tokens for this head\n",
    "#\n",
    "#   Example:\n",
    "#     query_states[0,0,:,:] = [[q0,q1], [q4,q5], [q8,q9]]  # head 0 across all tokens\n",
    "#     query_states[0,1,:,:] = [[q2,q3], [q6,q7], [q10,q11]]# head 1 across all tokens\n",
    "#\n",
    "# Keys and values go through the same reshape+transpose,\n",
    "# but with num_key_value_heads=4 in Llama (instead of 16 for queries).\n",
    "# So shapes end up:\n",
    "#   Q: [B, num_heads, S, head_dim]\n",
    "#   K: [B, num_kv_heads, S, head_dim]\n",
    "#   V: [B, num_kv_heads, S, head_dim]\n",
    "query_states = query_states.view(batch_size, sequence_length, num_attention_heads, head_dim).transpose(1, 2)\n",
    "key_states = key_states.view(batch_size, sequence_length, num_key_value_heads, head_dim).transpose(1, 2)\n",
    "value_states = value_states.view(batch_size, sequence_length, num_key_value_heads, head_dim).transpose(1, 2)\n",
    "\n",
    "print(\"Projected Shapes:\")\n",
    "print(f\"  query_states: {query_states.shape}\") # (batch_size, num_attention_heads, sequence_length, head_dim)\n",
    "print(f\"  key_states: {key_states.shape}\")     # (batch_size, num_key_value_heads, sequence_length, head_dim)\n",
    "print(f\"  value_states: {value_states.shape}\")   # (batch_size, num_key_value_heads, sequence_length, head_dim)\n",
    "\n",
    "num_key_value_groups = num_attention_heads // num_key_value_heads\n",
    "print(f\"\\nNum Key/Value Groups (Q heads per K/V head): {num_key_value_groups}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c5aac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Rotary Positional Embeddings (RoPE)\n",
    "# ---------------------------------------------------------------------------\n",
    "#\n",
    "# Transformers need to know the order of tokens in a sequence.\n",
    "# Instead of adding positional vectors (absolute embeddings),\n",
    "# RoPE injects positional information by ROTATING the Q and K vectors\n",
    "# before the dot product.\n",
    "#\n",
    "# Key idea:\n",
    "# - Each pair of dimensions in Q and K is treated like coordinates in 2D.\n",
    "# - We rotate them by an angle that depends on the token's position.\n",
    "# - Different dimension pairs rotate at different speeds (frequencies).\n",
    "#\n",
    "# Why?\n",
    "# - This encodes *relative position* information directly in Q and K.\n",
    "# - It improves performance, especially for long sequences.\n",
    "#\n",
    "# Implementation notes:\n",
    "# - Angles are computed from positions × frequencies.\n",
    "# - Cosine and sine represent the real/imag parts of the rotation (Euler’s formula).\n",
    "# - The result is stored as complex numbers (cos + i·sin).\n",
    "# - Later, these rotations are applied to Q and K in `apply_rotary_emb`.\n",
    "\n",
    "def simple_rope_calculation(dim, max_seq_len, base=10000.0, device=None):\n",
    "    \"\"\"\n",
    "    Simplified calculation of Rotary Positional Embedding (RoPE) frequencies.\n",
    "\n",
    "    Goal:\n",
    "    - Instead of adding position vectors, RoPE rotates Q and K vectors by an angle\n",
    "      that depends on the token’s position.\n",
    "    - These angles are created here and later applied to Q and K.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Create \"inverse frequencies\"\n",
    "    # Each pair of dimensions (0-1, 2-3, ...) gets a different rotation speed.\n",
    "    # Smaller indices → higher frequency, larger indices → lower frequency.\n",
    "    inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, device=device).float() / dim))\n",
    "\n",
    "    # Step 2: Positions of tokens (0,1,2,...,max_seq_len-1)\n",
    "    t = torch.arange(max_seq_len, device=device).type_as(inv_freq)\n",
    "\n",
    "    # Step 3: Outer product between positions and frequencies\n",
    "    # freqs[p, f] = position * frequency\n",
    "    # This gives us the rotation angle for each (position, frequency pair).\n",
    "    freqs = new_func(inv_freq, t)  # shape: [max_seq_len, dim/2]\n",
    "\n",
    "    # Step 4: Duplicate freqs to cover the full head dimension\n",
    "    # (because each pair of dims uses the same angle: one for cos, one for sin)\n",
    "    emb = torch.cat((freqs, freqs), dim=-1)  # shape: [max_seq_len, dim]\n",
    "\n",
    "    # Step 5: Compute cosine and sine\n",
    "    # cos = real part of rotation, sin = imaginary part\n",
    "    # Based on Euler’s formula: e^(iθ) = cos(θ) + i·sin(θ)\n",
    "    freqs_cos = emb.cos()  # shape [max_seq_len, dim]\n",
    "    freqs_sin = emb.sin()  # shape [max_seq_len, dim]\n",
    "\n",
    "    # Step 6: Combine into a complex number (cos + i·sin)\n",
    "    # This is the \"rotation operator\" we will apply to Q and K.\n",
    "    freqs_cis = torch.complex(freqs_cos, freqs_sin)  # shape: [max_seq_len, dim]\n",
    "\n",
    "    return freqs_cis\n",
    "\n",
    "\n",
    "def new_func(inv_freq, t):\n",
    "    # Outer product: multiplies every position with every frequency\n",
    "    # Example: if positions=[0,1,2] and freqs=[f0,f1],\n",
    "    # result = [[0*f0, 0*f1],\n",
    "    #           [1*f0, 1*f1],\n",
    "    #           [2*f0, 2*f1]]\n",
    "    return torch.outer(t, inv_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a1aac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_emb_torch(\n",
    "    xq: torch.Tensor,       # Queries: shape [batch, num_heads, seq_len, head_dim]\n",
    "    xk: torch.Tensor,       # Keys:    shape [batch, num_heads, seq_len, head_dim]\n",
    "    freqs_cis: torch.Tensor # Precomputed complex rotations: shape [max_seq_len, head_dim]\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Apply Rotary Positional Embeddings (RoPE) to Q and K.\n",
    "\n",
    "    Idea:\n",
    "    - Each pair of dimensions in Q and K is seen as a 2D vector.\n",
    "    - We rotate that 2D vector by an angle that depends on the token's position.\n",
    "    - Angles come from freqs_cis (cos + i·sin, precomputed).\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Move rotation frequencies to the same device (CPU/GPU) as Q\n",
    "    freqs_cis = freqs_cis.to(xq.device)\n",
    "\n",
    "    # 2. Pick the rotation angles for the actual token positions\n",
    "    #    Example: position_ids = [[0,1,2,...]] → selects rows 0,1,2... from freqs_cis\n",
    "    #    Result: [batch, seq_len, head_dim] (complex)\n",
    "    freqs_cis = freqs_cis[position_ids]\n",
    "\n",
    "    # 3. Add a \"heads\" axis for broadcasting\n",
    "    #    We want the same rotation applied to all heads of the same token\n",
    "    #    Shape becomes: [batch, 1, seq_len, head_dim] (complex)\n",
    "    freqs_cis = freqs_cis[:, None, :, :]\n",
    "\n",
    "    # ---------------- Prepare Q and K as complex numbers ----------------\n",
    "\n",
    "    # 4. Reshape last dim so that pairs of values become complex numbers\n",
    "    #    Example: [q0, q1] → q0 + i*q1\n",
    "    #    Shape: [B, H, S, head_dim] → [B, H, S, head_dim//2, 2] → complex → [B, H, S, head_dim//2]\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "\n",
    "    # ---------------- Prepare freqs_cis ----------------\n",
    "\n",
    "    # 5. Keep only the first half of freqs_cis (unique frequencies)\n",
    "    #    Because each complex number already represents a pair of dims\n",
    "    #    Shape: [B, 1, S, head_dim//2] (complex)\n",
    "    freqs_cis_broadcast = freqs_cis[..., :xq_.shape[-1]]\n",
    "\n",
    "    # ---------------- Apply the rotation ----------------\n",
    "\n",
    "    # 6. Rotate Q and K using element-wise complex multiplication\n",
    "    #    Multiplying by (cos + i·sin) applies the rotation\n",
    "    #    Broadcast: the '1' in freqs_cis_broadcast shares the same rotation across all heads\n",
    "    rotated_xq = xq_ * freqs_cis_broadcast\n",
    "    rotated_xk = xk_ * freqs_cis_broadcast\n",
    "\n",
    "    # ---------------- Convert back to real ----------------\n",
    "\n",
    "    # 7. Convert complex back to real pairs, then flatten\n",
    "    #    Shape: [B, H, S, head_dim//2] complex\n",
    "    #        -> view_as_real -> [B, H, S, head_dim//2, 2] real\n",
    "    #        -> flatten last two dims -> [B, H, S, head_dim] real\n",
    "    xq_out = torch.view_as_real(rotated_xq).flatten(3)\n",
    "    xk_out = torch.view_as_real(rotated_xk).flatten(3)\n",
    "\n",
    "    # 8. Cast back to the same dtype as input (e.g., float16 for efficiency)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36c41d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Precompute RoPE frequencies\n",
    "# --------------------------------------------------------------\n",
    "# RoPE works on the head_dim (per head), not the full hidden_size.\n",
    "# We generate rotation frequencies (cos + i·sin) for all possible positions\n",
    "# up to max_position_embeddings.\n",
    "freqs_cis = simple_rope_calculation(\n",
    "    head_dim,                  # dimension per head (e.g., 8)\n",
    "    max_position_embeddings,   # maximum sequence length (e.g., 256)\n",
    "    base=rope_theta,           # base frequency (default 10000.0)\n",
    "    device=hidden_states.device\n",
    ")\n",
    "print(f\"Calculated freqs_cis shape: {freqs_cis.shape}\")  \n",
    "# Expected: (max_position_embeddings, head_dim)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Apply RoPE to Q and K\n",
    "# --------------------------------------------------------------\n",
    "# Important: RoPE is applied BEFORE repeating K/V for grouped-query attention.\n",
    "# This rotates Q and K vectors by position-dependent angles,\n",
    "# injecting relative position information directly into them.\n",
    "query_states_rope, key_states_rope = apply_rotary_emb_torch(\n",
    "    query_states,   # shape [B, num_heads, S, head_dim]\n",
    "    key_states,     # shape [B, num_kv_heads, S, head_dim]\n",
    "    freqs_cis       # precomputed rotations\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Step 3: Check shapes after applying RoPE\n",
    "# --------------------------------------------------------------\n",
    "print(\"\\nShapes after RoPE:\")\n",
    "print(f\"  query_states_rope: {query_states_rope.shape}\")  # [B, num_heads, S, head_dim]\n",
    "print(f\"  key_states_rope:   {key_states_rope.shape}\")    # [B, num_kv_heads, S, head_dim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2f0bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Step 3: Optional Q/K Normalization (Simple L2 Norm)\n",
    "# --------------------------------------------------------------\n",
    "# Goal:\n",
    "# - Make sure Q and K vectors have a stable scale (not too big, not too small).\n",
    "# - This helps when we compute attention scores = Q @ K^T,\n",
    "#   so the softmax is stable (not exploding or vanishing).\n",
    "#\n",
    "# Idea:\n",
    "# - For each vector along the last dimension (head_dim),\n",
    "#   compute its size (RMS = sqrt(mean of squares)).\n",
    "# - Divide the vector by this size so its magnitude is ~1.\n",
    "# - Add a tiny epsilon (eps) to avoid division by zero.\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "class SimpleL2Norm(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps  # small constant for numerical stability\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [B, num_heads, seq_len, head_dim]\n",
    "\n",
    "        # 1. Compute mean of squares along the last dimension (head_dim)\n",
    "        #    keepdim=True → so result can broadcast back to x\n",
    "        norm = x.pow(2).mean(-1, keepdim=True)\n",
    "\n",
    "        # 2. Add epsilon and take reciprocal square root\n",
    "        #    torch.rsqrt(y) = 1 / sqrt(y)\n",
    "        scale = torch.rsqrt(norm + self.eps)\n",
    "\n",
    "        # 3. Multiply x by this scale → normalize each vector\n",
    "        return x * scale\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Apply normalization (if enabled)\n",
    "# --------------------------------------------------------------\n",
    "if use_qk_norm:\n",
    "    qk_norm = SimpleL2Norm()\n",
    "\n",
    "    # Normalize queries and keys after RoPE\n",
    "    query_states_final = qk_norm(query_states_rope)\n",
    "    key_states_final   = qk_norm(key_states_rope)\n",
    "\n",
    "    print(\"\\nApplied QK Norm\")  # confirms normalization was used\n",
    "else:\n",
    "    # Skip normalization: just use RoPE outputs\n",
    "    query_states_final = query_states_rope\n",
    "    key_states_final   = key_states_rope\n",
    "    print(\"\\nSkipped QK Norm\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Check final shapes before attention scores\n",
    "# --------------------------------------------------------------\n",
    "print(\"\\nShapes before attention score calculation:\")\n",
    "print(f\"  query_states_final: {query_states_final.shape}\")  # [B, num_heads, S, head_dim]\n",
    "print(f\"  key_states_final:   {key_states_final.shape}\")    # [B, num_kv_heads, S, head_dim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05486f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Step 4: Grouped-Query Attention (GQA) - Key/Value Repeating\n",
    "# --------------------------------------------------------------\n",
    "# Context:\n",
    "# - We already have Q, K, V after projection.\n",
    "# - But in GQA, we have MORE Q heads than K/V heads.\n",
    "#   Example: Q = 16 heads, K/V = 4 heads.\n",
    "# - Solution: \"repeat\" each K/V head enough times so every Q head\n",
    "#   has a matching K and V.\n",
    "# - After repeating:\n",
    "#     Q: [B, 16, S, head_dim]\n",
    "#     K: [B, 16, S, head_dim]  <-- repeated from 4 heads\n",
    "#     V: [B, 16, S, head_dim]  <-- repeated from 4 heads\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Repeat Key/Value heads for GQA.\n",
    "\n",
    "    Args:\n",
    "        hidden_states: tensor of shape [B, num_kv_heads, S, head_dim]\n",
    "        n_rep: how many times each K/V head should be repeated\n",
    "               (num_attention_heads // num_kv_heads)\n",
    "\n",
    "    Returns:\n",
    "        repeated tensor of shape [B, num_attention_heads, S, head_dim]\n",
    "    \"\"\"\n",
    "    batch, num_kv_heads, slen, head_dim = hidden_states.shape\n",
    "\n",
    "    if n_rep == 1:\n",
    "        # No repeating needed (KV heads == Q heads)\n",
    "        return hidden_states\n",
    "\n",
    "    # Step 1: Insert a new axis after kv_heads for the repetitions\n",
    "    # Shape becomes: [B, num_kv_heads, 1, S, head_dim]\n",
    "    hidden_states = hidden_states[:, :, None, :, :]\n",
    "\n",
    "    # Step 2: Expand along the new axis to repeat each KV head n_rep times\n",
    "    # Shape: [B, num_kv_heads, n_rep, S, head_dim]\n",
    "    hidden_states = hidden_states.expand(batch, num_kv_heads, n_rep, slen, head_dim)\n",
    "\n",
    "    # Step 3: Merge kv_heads * n_rep into one dimension\n",
    "    # Final shape: [B, num_kv_heads * n_rep, S, head_dim]\n",
    "    return hidden_states.reshape(batch, num_kv_heads * n_rep, slen, head_dim)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Apply repeating to K and V\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "# Repeat K: normalized and RoPE-applied\n",
    "key_states_repeated = repeat_kv(key_states_final, num_key_value_groups)\n",
    "\n",
    "# Repeat V: use original V (no RoPE, no norm applied to values)\n",
    "value_states_repeated = repeat_kv(value_states, num_key_value_groups)\n",
    "\n",
    "print(\"\\nShapes after repeating K/V for GQA:\")\n",
    "print(f\"  key_states_repeated:   {key_states_repeated.shape}\")   # should match Q heads\n",
    "print(f\"  value_states_repeated: {value_states_repeated.shape}\") # should match Q heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a32bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Step 5: Scaled Dot-Product Attention\n",
    "# --------------------------------------------------------------\n",
    "# Context recap:\n",
    "# - We now have:\n",
    "#     Q = query_states_final        # [B, num_heads, S, head_dim]\n",
    "#     K = key_states_repeated       # [B, num_heads, S, head_dim]  (after GQA repeat)\n",
    "#     V = value_states_repeated     # [B, num_heads, S, head_dim]  (after GQA repeat)\n",
    "# - We also have a causal attention_mask: [B, 1, S, S]\n",
    "# - Goal: weights = softmax((Q @ K^T) / sqrt(d)) ; output = weights @ V\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "# 1) Raw attention scores = Q @ K^T\n",
    "#    For each batch and head, compare every query token (S) against every key token (S).\n",
    "#    Shapes:\n",
    "#      Q: [B, H, S, D]\n",
    "#      K^T: transpose last two dims → [B, H, D, S]\n",
    "#      result: [B, H, S, S]  (score of each query token vs each key token)\n",
    "attn_weights = torch.matmul(query_states_final, key_states_repeated.transpose(2, 3))\n",
    "\n",
    "# 2) Scale by 1 / sqrt(head_dim)\n",
    "#    Reason: keep magnitudes stable so softmax doesn’t become too peaky or too flat.\n",
    "scaling_factor = 1.0 / math.sqrt(head_dim)\n",
    "attn_weights = attn_weights * scaling_factor\n",
    "\n",
    "# 3) Apply causal mask (no looking ahead)\n",
    "#    Mask shape is [B, 1, S, S] and broadcasts across heads.\n",
    "#    We slice mask’s key-length dimension to match K/V sequence length if needed.\n",
    "if attention_mask is not None:\n",
    "    print(f\"\\nApplying attention mask with shape: {attention_mask.shape}\")\n",
    "    causal_mask = attention_mask[:, :, :, :key_states_repeated.shape[-2]]  # ensure key-dim match\n",
    "    # Add large negative values (-inf) to forbidden positions so softmax → 0 there.\n",
    "    attn_weights = attn_weights + causal_mask\n",
    "else:\n",
    "    print(\"\\nNo attention mask applied.\")\n",
    "\n",
    "# 4) Softmax over keys dimension\n",
    "#    Turn scores into probabilities along the last dim (which indexes keys).\n",
    "#    Result stays [B, H, S, S].\n",
    "attn_weights = nn.functional.softmax(attn_weights, dim=-1).to(query_states.dtype)\n",
    "\n",
    "# 5) (Optional) Dropout on attention weights (used in training; skipped here for inference)\n",
    "# attn_weights = nn.functional.dropout(attn_weights, p=attention_dropout, training=self.training)\n",
    "\n",
    "# 6) Weighted sum of Values → attention output\n",
    "#    Multiply probabilities [B, H, S, S] by V [B, H, S, D] along the key/token axis.\n",
    "#    Each query token gets a weighted combination of all value tokens → [B, H, S, D].\n",
    "attn_output = torch.matmul(attn_weights, value_states_repeated)\n",
    "\n",
    "print(\"\\nAttention Calculation Shapes:\")\n",
    "print(f\"  attn_weights (after mask+softmax): {attn_weights.shape}\")  # [B, H, S, S]\n",
    "print(f\"  attn_output: {attn_output.shape}\")                         # [B, H, S, D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac083a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Step 6: Reshape and Output Projection\n",
    "# --------------------------------------------------------------\n",
    "# Context recap:\n",
    "# - After Step 5, attn_output = [B, num_heads, S, head_dim]\n",
    "#   Each head produced its own [S, head_dim] output.\n",
    "# - Next steps:\n",
    "#   1) Concatenate all heads together (merge num_heads * head_dim).\n",
    "#   2) Pass through a final linear layer (o_proj) to map back\n",
    "#      to hidden_size, so the rest of the Transformer sees the\n",
    "#      same shape it started with.\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "# 1) Move heads dimension after sequence\n",
    "#    From [B, H, S, D] → [B, S, H, D]\n",
    "attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "\n",
    "# 2) Flatten heads into one big vector per token\n",
    "#    Shape: [B, S, H*D] = [B, S, hidden_size]\n",
    "attn_output = attn_output.view(batch_size, sequence_length, hidden_size)\n",
    "\n",
    "# 3) Final linear projection back to hidden_size\n",
    "#    This mixes information across heads and ensures output\n",
    "#    has the same dimension as the model embedding (hidden_size).\n",
    "final_attn_output = o_proj(attn_output)\n",
    "\n",
    "print(\"\\nFinal Output Shapes:\")\n",
    "print(f\"  attn_output (reshaped): {attn_output.shape}\")       # [B, S, hidden_size]\n",
    "print(f\"  final_attn_output: {final_attn_output.shape}\")      # [B, S, hidden_size]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
