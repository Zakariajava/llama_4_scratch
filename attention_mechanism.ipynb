{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300c4e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7c07337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Number of GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA available:\", torch.cuda.is_available())\n",
    "    print(\"Number of GPUs:\", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba335b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration --- # \n",
    "\n",
    "hidden_size = 128\n",
    "# Dimension of the embedding vector for each token.\n",
    "# Example: every token (like \"dog\") is represented by a vector of 128 numbers.\n",
    "\n",
    "num_attention_heads = 16\n",
    "# How many attention heads we use in parallel.\n",
    "# The hidden_size (128) is split across these heads.\n",
    "# Each head sees only part of the vector → 128 / 16 = 8 dimensions per head.\n",
    "\n",
    "num_key_value_heads = 4\n",
    "# Special trick: Grouped-Query Attention (GQA).\n",
    "# Instead of creating 16 different Key/Value heads,\n",
    "# we only create 4 K/V heads (each of size 8) and let multiple Q heads share them.\n",
    "# → This saves memory and computation while keeping good performance.\n",
    "\n",
    "head_dim = hidden_size // num_attention_heads\n",
    "# Size of each head’s Q, K, V vector.\n",
    "# With hidden_size=128 and 16 heads, each head works in 8 dimensions.\n",
    "\n",
    "max_position_embeddings = 256\n",
    "# Maximum sequence length (number of tokens) the model can process at once.\n",
    "# If a sentence has more than 256 tokens, it must be truncated or split.\n",
    "\n",
    "rope_theta = 10000.0\n",
    "# Base frequency for Rotary Position Embeddings (RoPE).\n",
    "# It controls how positional information is encoded.\n",
    "# Larger theta = slower change in frequency = smoother positional encoding.\n",
    "\n",
    "rms_norm_eps = 1e-5\n",
    "# Tiny constant added inside RMSNorm to avoid division by zero.\n",
    "# Ensures stability in training and inference.\n",
    "\n",
    "attention_bias = False\n",
    "# Whether to add a bias term to the linear layers that produce Q, K, V.\n",
    "# Usually kept False for efficiency.\n",
    "\n",
    "attention_dropout = 0.0\n",
    "# Dropout probability applied to attention weights (to prevent overfitting).\n",
    "# Often set to 0.0 during inference (disabled).\n",
    "\n",
    "use_qk_norm = True\n",
    "# Whether to normalize Q and K vectors (L2 norm) before computing attention scores.\n",
    "# This keeps dot products more stable and avoids extreme attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5071a1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  hidden_size: 128\n",
      "  num_attention_heads: 16\n",
      "  num_key_value_heads: 4\n",
      "  head_dim: 8\n",
      "\n",
      "Sample Input Shapes:\n",
      "  hidden_states: torch.Size([2, 10, 128])\n",
      "  position_ids: torch.Size([2, 10])\n",
      "  attention_mask: torch.Size([2, 1, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "# --- Sample input setup ---\n",
    "\n",
    "batch_size = 2  \n",
    "# Number of independent sequences (context windows) processed in parallel.\n",
    "# Example: 2 separate sentences.\n",
    "\n",
    "sequence_length = 10  \n",
    "# Number of tokens in each sequence (the length of the context window).\n",
    "\n",
    "hidden_states = torch.randn(batch_size, sequence_length, hidden_size)  \n",
    "# Random embeddings for each token in each sequence.\n",
    "# Shape = (batch_size, sequence_length, hidden_size)\n",
    "#        = (2, 10, 128)\n",
    "# Meaning:\n",
    "# - 2 sequences\n",
    "# - Each sequence has 10 tokens\n",
    "# - Each token is represented by a 128-dimensional vector\n",
    "\n",
    "# --- Position IDs creation ---\n",
    "\n",
    "position_ids = torch.arange(0, sequence_length).unsqueeze(0).repeat(batch_size, 1)\n",
    "# position_ids = torch.arange(0, sequence_length)  \n",
    "# → Shape: (10,)\n",
    "# → [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# position_ids = position_ids.unsqueeze(0)  \n",
    "# Add a new dimension at the front\n",
    "# → Shape: (1, 10)\n",
    "# → [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]\n",
    "\n",
    "# position_ids = position_ids.repeat(batch_size, 1)  \n",
    "# Repeat the row for each sequence in the batch\n",
    "# → Shape: (2, 10)\n",
    "# → [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "#    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]\n",
    "\n",
    "# Intuition:\n",
    "# - Each token in each sequence needs a position ID.\n",
    "# - Both sequences start at position 0, because they are independent windows.\n",
    "\n",
    "# Create a causal attention mask\n",
    "# Goal: make sure each token can only see itself and tokens before it (no looking into the future)\n",
    "\n",
    "attention_mask = torch.triu(torch.ones(sequence_length, sequence_length) * -torch.inf, diagonal=1)\n",
    "# Step 1: Make a square matrix (seq x seq).\n",
    "# -∞ above the diagonal = future tokens (blocked)\n",
    "# 0 on and below diagonal = current/past tokens (allowed)\n",
    "\n",
    "attention_mask = attention_mask.unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, seq, seq)\n",
    "# Step 2: Add two extra dimensions so the mask matches attention shapes.\n",
    "# Now we have 4D: [1, 1, seq, seq]\n",
    "\n",
    "attention_mask = attention_mask.expand(batch_size, 1, -1, -1)  # Shape: (batch, 1, seq, seq)\n",
    "# Step 3: Copy the mask for each sequence in the batch.\n",
    "# The \"1\" in heads dimension means the same mask is shared across all attention heads.\n",
    "print(\"Configuration:\")\n",
    "print(f\"  hidden_size: {hidden_size}\")\n",
    "print(f\"  num_attention_heads: {num_attention_heads}\")\n",
    "print(f\"  num_key_value_heads: {num_key_value_heads}\")\n",
    "print(f\"  head_dim: {head_dim}\")\n",
    "\n",
    "print(\"\\nSample Input Shapes:\")\n",
    "print(f\"  hidden_states: {hidden_states.shape}\")\n",
    "print(f\"  position_ids: {position_ids.shape}\")\n",
    "print(f\"  attention_mask: {attention_mask.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eb2aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Q, K, V Projections\n",
    "#\n",
    "# The first step in attention is to project the input `hidden_states` into Query (Q), Key (K), and Value (V) representations using linear layers.\n",
    "#\n",
    "# - **Q:** Represents the current token's query.\n",
    "# - **K:** Represents the keys of all tokens in the sequence (or context).\n",
    "# - **V:** Represents the values (information) of all tokens.\n",
    "#\n",
    "# Llama 4 uses Grouped-Query Attention (GQA). This means there are fewer K and V heads than Q heads. The `num_key_value_groups` tells us how many Q heads share the same K and V head. This reduces computation and memory requirements.\n",
    "\n",
    "# %%\n",
    "# Define projection layers\n",
    "q_proj = nn.Linear(hidden_size, num_attention_heads * head_dim, bias=attention_bias)\n",
    "k_proj = nn.Linear(hidden_size, num_key_value_heads * head_dim, bias=attention_bias)\n",
    "v_proj = nn.Linear(hidden_size, num_key_value_heads * head_dim, bias=attention_bias)\n",
    "o_proj = nn.Linear(num_attention_heads * head_dim, hidden_size, bias=attention_bias)\n",
    "\n",
    "# Calculate projections\n",
    "query_states = q_proj(hidden_states)\n",
    "key_states = k_proj(hidden_states)\n",
    "value_states = v_proj(hidden_states)\n",
    "\n",
    "# Reshape Q, K, V for multi-head attention\n",
    "# Target shape: (batch_size, num_heads, sequence_length, head_dim)\n",
    "query_states = query_states.view(batch_size, sequence_length, num_attention_heads, head_dim).transpose(1, 2)\n",
    "key_states = key_states.view(batch_size, sequence_length, num_key_value_heads, head_dim).transpose(1, 2)\n",
    "value_states = value_states.view(batch_size, sequence_length, num_key_value_heads, head_dim).transpose(1, 2)\n",
    "\n",
    "\n",
    "print(\"Projected Shapes:\")\n",
    "print(f\"  query_states: {query_states.shape}\") # (batch_size, num_attention_heads, sequence_length, head_dim)\n",
    "print(f\"  key_states: {key_states.shape}\")     # (batch_size, num_key_value_heads, sequence_length, head_dim)\n",
    "print(f\"  value_states: {value_states.shape}\")   # (batch_size, num_key_value_heads, sequence_length, head_dim)\n",
    "\n",
    "num_key_value_groups = num_attention_heads // num_key_value_heads\n",
    "print(f\"\\nNum Key/Value Groups (Q heads per K/V head): {num_key_value_groups}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
